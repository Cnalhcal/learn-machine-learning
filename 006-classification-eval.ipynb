{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from my_ml_package.visualize import plot_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Accuracy NOT Accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Accuracy=====\n",
      "2 non-cancer (0) -> cancer (1): 6/8= 0.75\n",
      "1 cancer (1) ->  non-cancer (0): 7/8= 0.875\n",
      "Better Performance: 1 cancer (1) ->  non-cancer (0)\n",
      "\n",
      "\n",
      "=====Recall=====\n",
      "2 non-cancer (0) -> cancer (1): 4/4= 1.0\n",
      "1 cancer (1) ->  non-cancer (0): 3/4= 0.75\n",
      "Better Performance: 2 non-cancer (0) -> cancer (1)\n"
     ]
    }
   ],
   "source": [
    "# 1 represents the cancer people; 0 represents the non-cancer people\n",
    "y =                       [0, 0, 0, 0, 1, 1, 1, 1] \n",
    "\n",
    "# 2 non-cancer (0) -> cancer (1), defined as FP (False Positive)\n",
    "y_2non_cancer_to_cancer = [1, 1, 0, 0, 1, 1, 1, 1] \n",
    "#                          *  * \n",
    "# 1 cancer (1) ->  non-cancer (0), defined as FN (False Negative)\n",
    "y_1cancer_to_non_cancer = [0, 0, 0, 0, 1, 0, 1, 1]   \n",
    "#                                         *\n",
    "print('=====Accuracy=====')\n",
    "print('2 non-cancer (0) -> cancer (1): 6/8=', metrics.accuracy_score(y, y_2non_cancer_to_cancer))\n",
    "print('1 cancer (1) ->  non-cancer (0): 7/8=', metrics.accuracy_score(y, y_1cancer_to_non_cancer))\n",
    "print('Better Performance: 1 cancer (1) ->  non-cancer (0)')\n",
    "print('\\n')\n",
    "\n",
    "print('=====Recall=====')\n",
    "print('2 non-cancer (0) -> cancer (1): 4/4=', metrics.recall_score(y, y_2non_cancer_to_cancer))\n",
    "print('1 cancer (1) ->  non-cancer (0): 3/4=', metrics.recall_score(y, y_1cancer_to_non_cancer))\n",
    "print('Better Performance: 2 non-cancer (0) -> cancer (1)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if the disease it detects is “CoVid” while there is a lack of doctors for diagnosis? \n",
    "  * So we are more likely to favour FP rather than FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Accuracy=====\n",
      "2 non-covid (0) -> covid (1): 10/12= 0.8333333333333334\n",
      "3 covid (1) ->  non-covid (0): 9/12= 0.75\n",
      "Better Performance: 2 non-covid (0) -> covid (1)\n",
      "\n",
      "\n",
      "=====Recall=====\n",
      "2 non-covid (0) -> covid (1): 8/8= 1.0\n",
      "3 covid (1) ->  non-covid (0): 5/8= 0.625\n",
      "Better Performance: 2 non-covid (0) -> covid (1)\n",
      "\n",
      "\n",
      "=====Precision=====\n",
      "2 non-covid (0) -> covid (1): 8/10= 0.8\n",
      "1 covid (1) ->  non-covid (0): 5/5= 1.0\n",
      "Better Performance: 3 covid (1) ->  non-covid (0)\n"
     ]
    }
   ],
   "source": [
    "# 1 represents the covid people; 0 represents the non-covid people\n",
    "y =                       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# 2 non-covid (0) -> covid (1)\n",
    "y_3non_covid_mispredict = [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1] # 2 non-covid -> covid\n",
    "#                          *  *\n",
    "# 3 covid (0) -> no covid (1)\n",
    "y_2covid_mispredict =     [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]  \n",
    "#                                      *  *  *\n",
    "\n",
    "print('=====Accuracy=====')\n",
    "print('2 non-covid (0) -> covid (1): 10/12=', metrics.accuracy_score(y, y_3non_covid_mispredict))\n",
    "print('3 covid (1) ->  non-covid (0): 9/12=', metrics.accuracy_score(y, y_2covid_mispredict))\n",
    "print('Better Performance: 2 non-covid (0) -> covid (1)')\n",
    "print('\\n')\n",
    "print('=====Recall=====')\n",
    "print('2 non-covid (0) -> covid (1): 8/8=', metrics.recall_score(y, y_3non_covid_mispredict))\n",
    "print('3 covid (1) ->  non-covid (0): 5/8=', metrics.recall_score(y, y_2covid_mispredict))\n",
    "print('Better Performance: 2 non-covid (0) -> covid (1)')\n",
    "print('\\n')\n",
    "print('=====Precision=====')\n",
    "print('2 non-covid (0) -> covid (1): 8/10=', metrics.precision_score(y, y_3non_covid_mispredict))\n",
    "print('1 covid (1) ->  non-covid (0): 5/5=', metrics.precision_score(y, y_2covid_mispredict))\n",
    "print('Better Performance: 3 covid (1) ->  non-covid (0)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizing Precision and Recall using Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The two metrics are called [Recall and Precision](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "    + We want to correctly detect actually Covid patients, i.e., aiming to decrease FN\n",
    "    + Recall $\\frac{TP}{TP+FN}$  (Sensitivity or TPR): hit rate.\n",
    "   \n",
    "    + but also want to reflect FP: false alarm (incorrectly detecting healthy patients as having Covid)\n",
    "    + Precision $\\frac{TP}{TP+FP}$\n",
    "    <!-- + Other metrics reflecting FP: ?? -->\n",
    "    <!-- Specificity=$\\frac{TN}{N}=\\frac{TN}{TN+FP}$ -->\n",
    "    + Opposite directions: the larger the metrics, the more FN/FP\n",
    "    <!-- missing rate: $\\frac{FN}{TP+FP}$ -->\n",
    "    <!-- FPR=$\\frac{FP}{N}=\\frac{FP}{TN+FP}$\n",
    "\n",
    "<!-- precision: how many examples are correctly predicted during all predictions for a target\n",
    "recall: how many examples are correctly predicted during all examples of one class -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#               pred0     pred1\n",
    "# actual 0       [[TN,      ],\n",
    "# actual 1        [,        ]]\n",
    "\n",
    "# formulate Accuracy by Confusion Matrix?\n",
    "\n",
    "# confusion_matrix = metrics.confusion_matrix(y, y_pred1)\n",
    "# print(confusion_matrix)\n",
    "# plot_cm(y, y_pred, labels=['0', '1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Recall for FP predictions: ', metrics.recall_score(y, y_pred1))\n",
    "# print('Precision for FP predictions: ', metrics.precision_score(y, y_pred1))\n",
    "\n",
    "# print('Recall for FN predictions: ', metrics.recall_score(y, y_pred2))\n",
    "# print('Precision for FN predictions: ', metrics.precision_score(y, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* F1: Balancing Recall and Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(metrics.classification_report(y, y_pred, labels=[0, 1], target_names=[\"zero\",\"one\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How we can reduce misclassification for one class of examples? \n",
    "    + we always have a tradeoff between FN and FP. since, in reality, the model will output probabilities rather than directly give the class.\n",
    "*  How would reducing misclassification for one class of examples likely increase misclassification for another class of examples?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I take the demonstrated example from https://www.youtube.com/watch?v=4jRBRDbJemM&t=655s\n",
    "y =      np.array([0,   0,   0,   0,   1,   1,   1,   1])\n",
    "y_prob = np.array([0.5, 0.2, 0,   0,   0.5, 1,   1,   1])\n",
    "y_pred = y_prob >= 0.5\n",
    "print(y_pred)\n",
    "print(\"FN\", metrics.confusion_matrix(y, y_pred)[1,0])\n",
    "print(\"FP\", metrics.confusion_matrix(y, y_pred)[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROC (Receiver Operating Characteristic) Curve reflecting the trade-off between FN and FP \n",
    "    + True Positive Rate (TPR, i.e., recall): 1 when FN is 0\n",
    "    + False Positive Rate (FPR): 0 when FP is 0; Which metric can be used to replace FPR?\n",
    "    + So the best point is (FPR, TPR) = (0, 1)\n",
    "    + (0, 0) is the extreme model for reducing FP\n",
    "    + (1, 1) is the extreme model for reducing FN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y, y_prob)\n",
    "print(thresholds)\n",
    "# Calculate AUC\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, marker='o', label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random guessing')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But they have different significaces in the real world.\n",
    "* In which cases we do not want people with disease midclassified as no disease, i.e., less FN?\n",
    "    + life threathening scenarios, e.g., heart disease, cancer\n",
    "    + disease easily causing outbreak, e.g., Covid\n",
    "    \n",
    "    \n",
    "* In which cases we do not want healthy people misclassified as with disease, i.e., less FP\n",
    "    + small problems causing unnecessary hospital cost, e.g., diarrhea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

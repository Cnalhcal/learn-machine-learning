{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* The common transformations can be divided into two categories: \n",
    "    + functions (no parameters during learning process):`pytorch.functional` contains the basic functions for high-level object-oriented modules with `torch.Tensor` (matrices) as parameters\n",
    "    + architectures (containing learnable parameters): Rather than build neural network in a neuron/connection level, deep learning frameworks build NN in the **layer** level and use Object-oriented Implementations of Architectures. Two popular ones are Tensorflow `tensorflow.keras.layers` (e.g., `Conv1D`) and Pytorch `torch.nn.Module` (e.g., `Linear`, `Conv1d`, `Dropout`).\n",
    "    + Pros/Cons of using layer-level architectures: only need to care inputs/outputs of each layers; unknown design decision\n",
    "    + How to trace the parameters ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How pytorch accumulate gradients for a batch of samples （e.g., [x1, x2]）independently in a vectorized way? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parameters\n",
    "w = torch.Tensor([1.,2.,3.])\n",
    "print('Original w grad:', w.grad)\n",
    "\n",
    "\n",
    "# we create `x` containing two examples, each has 3-dimensional features\n",
    "x1 = torch.Tensor([1., 2., 3.])\n",
    "x2 = torch.Tensor([4., 5., 6.])\n",
    "\n",
    "y = x1 * w\n",
    "z = y.sum()\n",
    "z.backward()\n",
    "print('w grad with x1:', w.grad)\n",
    "\n",
    "y = x2 * w\n",
    "z = y.sum()\n",
    "w.grad = None\n",
    "z.backward()\n",
    "print('w grad with x2:', w.grad)\n",
    "\n",
    "\n",
    "# The gradients will be summed up in the batch dimension\n",
    "x = torch.Tensor([[1., 2., 3.],\n",
    "                  [4., 5., 6.]])\n",
    "y = x * w\n",
    "z = y.sum()\n",
    "w.grad = None\n",
    "z.backward()\n",
    "print('mini-batch gradient of parameters with x1 and x2:', w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level neural network using `nn.Module`\n",
    "\n",
    "As said above, a neural network is just a stack of operations on data input tensors and model parameter tensors. `nn.Module` has the basic implementation to record the model parameters and operations in high level.\n",
    "\n",
    "In a nutshell, \n",
    "\n",
    "**All the neural networks in Pytorch are built upon the parent class `nn.Module`**\n",
    "\n",
    "The following code cell demonstrates how model parameters are used by Linear module `class Linear(Module)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Use Pytorch Linear Module\n",
    "nn_module = nn.Linear(5, 2)\n",
    "for p in nn_module.parameters():\n",
    "    print('W or b: ', p.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Module` is used **in a nested way**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build customized pytorch nn modules\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # pytorch will register layers and operations we put into the network\n",
    "        \n",
    "        self.hidden = nn.Linear(784, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define neural network \n",
    "mlp = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2914, 1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(1024, 512),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(512, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 5),\n",
    ")\n",
    "\n",
    "# define optimizer for gradient descent\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.001)\n",
    "\n",
    "# assign the class weights according to the number of samples\n",
    "# in each class in X\n",
    "class_weights = [X_train.shape[0]/np.sum(y_train==i) for i in range(len(target_names))]\n",
    "# class_weights = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "    outputs = mlp(X_train_tensor)\n",
    "    loss = F.cross_entropy(outputs, y_train_tensor, weight=class_weights)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print('Epoch: ', epoch, ' Loss: ', loss.item())\n",
    "\n",
    "# Predict\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "y_pred = mlp(X_test_tensor).argmax(dim=1).numpy()\n",
    "\n",
    "# Evaluate\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, make_scorer\n",
    "from sklearn.tree import DecisionTreeClassifier as skDecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.tree import plot_tree\n",
    "from my_ml_package.decision_tree import DecisionTreeClassifier\n",
    "from my_ml_package.data.synthetic import variance_sample_for_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delibrarely Generating A High-Variance Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.65\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, y_train = variance_sample_for_decision_tree(sample_size = 40) # we need some variabiliy in the data for bootstrap to work\n",
    "X_test, y_test = variance_sample_for_decision_tree(sample_size = 2000)\n",
    "\n",
    "treeclf = DecisionTreeClassifier()\n",
    "treeclf.fit(X_train, y_train)\n",
    "y_train_pred = treeclf.predict(X_train)\n",
    "y_pred = treeclf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('The accuracy on test data is {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "# print('The test error rate is {:.2f}'.format(1 - accuracy_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "# The accuracy on training data is 0.85\n",
    "# The accuracy on test data is 0.69\n",
    "\n",
    "# 0.37\n",
    "# 0.16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with High Variance "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training an Ensemble model: train different independent models on slightly different subsets of data\n",
    "* How to make each model independent with others? \n",
    "* Hint: The way the data is fed into the models can be challenging\n",
    "\n",
    "## Bagging\n",
    "* Trained Multiple Models on Bootstrap datasets\n",
    "    + Bootstrap: Resampling the same size of sample with replacement; reduce variance\n",
    "    + Bootstrap AGGregation (BAGGing): agggregate the prediction over a collection of bootstrap samples\n",
    "    + A Bootstrap sample $\\mathbf{Z}^{* b}, b=1,2, \\ldots, B$ -> a fitted model $\\hat{f}^{* b}(x)$\n",
    "        $$\\hat{f}_{\\mathrm{bag}}(x)=\\frac{1}{B} \\sum_{b=1}^B \\hat{f}^{* b}(x)$$\n",
    "\n",
    "Bagging Inference\n",
    "* Voting for classification: $\\hat{G}_{\\text {bag }}(x)=\\arg \\max _k \\hat{f}_{\\mathrm{bag}}(x)$\n",
    "    + \"It is tempting to treat the voting proportions pk(x) as estimates of these probabilities. A simple two-class example shows that they fail in this regard.\" (Hastie, 2008) **How/why fails?**\n",
    "     <!--  Suppose the true probability of class 1 at x is 0.75, and each of the bagged classifiers accurately predict a 1. Then p1(x) = 1, which is incorrect. -->\n",
    "    + \"An alternative bagging strategy is to average these instead, rather than the vote indicator vectors.\"\n",
    "* Averaging for regression \n",
    "* Out-of-bag samples: about 1/3 original data is not in the bootstrap dataset which can be used for model evaluation\n",
    "\n",
    "Goal: reduce the variance of unstable (high variance) learning methods. Assuming that the variables are simply i.d. (identically distributed, but not necessarily independent) with positive pairwise correlation œÅ, the variance\n",
    "of the average is\n",
    "\\begin{equation}\n",
    "\\rho \\sigma^2+\\frac{1-\\rho}{B} \\sigma^2\n",
    "(\\#eq:variance)\n",
    "\\end{equation}\n",
    "\n",
    "<!-- Question 1: Which learning model/method is ideal for bagging?\n",
    "\n",
    "Question 2: Will it reduce bias?\n",
    "\n",
    "\"since each tree generated in bagging is [identically distributed (i.d.)](https://stats.stackexchange.com/questions/89036/why-the-trees-generated-via-bagging-are-identically-distributed#:~:text=Bagging%20technique%20uses%20bootstraps%20\\(random,population%20as%20the%20original%20sample.), the expectation of an average of B such trees is the same as the expectation of any one of them. This means the bias of bagged trees is the same as that of the individual trees, and the only hope of improvement is through variance reduction. This is in contrast to boosting, where the trees are grown in an adaptive way to remove bias, and hence are not i.d.\" (Hastie, 2008) -->\n",
    "\n",
    "\n",
    "<!-- <center><img src=\"pics/bagging.png\" width=\"500\"></center>\n",
    "\n",
    "<center><img src=\"pics/bagging_result.png\" width=\"500\"></center>\n",
    "\n",
    "* the trees have high variance due to the correlation in the predictors\n",
    "* Bagging succeeds in smoothing out this variance and hence reducing the test error\n",
    "    + \" averaging reduces variance and leaves bias unchanged\" (Hastie, 2008) -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- ## [Random Forest](https://link.springer.com/article/10.1023/A:1010933404324)\n",
    "* \"the size of the correlation of pairs of bagged trees limits the benefits of averaging\" according to Formula 1\n",
    "* \"a substantial modification of bagging that builds a large collection of de-correlated trees\"\n",
    "* Iteratively 1) make a bootstrapped dataset; 2) only use a random subset of variables at each splitting (`max_features`)\n",
    "* can handle large data sets with higher dimensionality (thousands of input variables).\n",
    "* can identify most significant variables -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 0.78\n",
      "The accuracy on test data is 0.50\n",
      "The accuracy on training data is 0.88\n",
      "The accuracy on test data is 0.61\n"
     ]
    }
   ],
   "source": [
    "# bootstrap sample\n",
    "def boostrap(X, y):\n",
    "    \"\"\" Returns a boostrap sample of the data X, y.\n",
    "\n",
    "    returns:\n",
    "    X_boot: np.array, shape (n_samples, n_features)\n",
    "    y_boot: np.array, shape (n_samples, )\n",
    "    \"\"\"\n",
    "    X_boot = []\n",
    "    y_boot = []\n",
    "    train_size = X.shape[0]\n",
    "    for _ in range(train_size):\n",
    "        idx = np.random.randint(0, len(X))\n",
    "        X_boot.append(X[idx])\n",
    "        y_boot.append(y[idx])\n",
    "    # to numpy array\n",
    "    return np.array(X_boot), np.array(y_boot)\n",
    "\n",
    "# y_true = 0 # 3-class\n",
    "# Orig: 30  -> Orig tree     y_test = 1 \n",
    "# Bootstrap 1: 30 -> tree 1  y_test = 0\n",
    "# Bootstrap 2: 30 -> tree 2  y_test = 1\n",
    "# Bootstrap 3: 30 -> tree 3  y_test = 2\n",
    "# Bootstrap 4: 30 -> tree 4  y_test = 0\n",
    "# Bootstrap 5: 30 -> tree 5  y_test = 0\n",
    "# 1: 2\n",
    "# 0: 3\n",
    "\n",
    "n_estimators = 2000\n",
    "max_depth = 2\n",
    "criterion = \"entropy\"\n",
    "ensemble_clf = []\n",
    "for _ in range(n_estimators):\n",
    "    # bootstrap sample\n",
    "    X_boot, y_boot = boostrap(X_train, y_train)\n",
    "\n",
    "    # fit an ensemble of classification trees\n",
    "    clf = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    clf.fit(X_boot, y_boot)\n",
    "    ensemble_clf.append(clf)\n",
    "   \n",
    "# predict the test data\n",
    "y_preds = []\n",
    "for clf in ensemble_clf:\n",
    "    y_preds.append(clf.predict(X_test))\n",
    "y_pred = (np.array(y_preds).mean(axis=0) > 0.5).astype(int)\n",
    "y_train_preds = []\n",
    "for clf in ensemble_clf:\n",
    "    y_train_preds.append(clf.predict(X_train))\n",
    "y_train_pred = (np.array(y_train_preds).mean(axis=0) > 0.5).astype(int)\n",
    "\n",
    "print('The accuracy on training data is {:.2f}'.format(accuracy_score(y_train, y_train_pred)))\n",
    "print('The accuracy on test data is {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "# sklearn implementation\n",
    "rf_clf = RandomForestClassifier(n_estimators=n_estimators, criterion=criterion, max_features=None, max_depth=max_depth)   \n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(rf_clf.score(X_train, y_train)))\n",
    "print('The accuracy on test data is {:.2f}'.format(rf_clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# for estimator in rf_clf.estimators_:\n",
    "#     plt.figure(figsize=(12,12))\n",
    "#     tree.plot_tree(estimator, filled=True, rounded=True)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 0.85\n",
      "The accuracy on test data is 0.75\n"
     ]
    }
   ],
   "source": [
    "# sklearn implementation\n",
    "n_estimators = 2000\n",
    "rf_clf = RandomForestClassifier(n_estimators=n_estimators, criterion=\"entropy\", max_features=None, max_depth=2)   \n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(rf_clf.score(X_train, y_train)))\n",
    "print('The accuracy on test data is {:.2f}'.format(rf_clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "# for estimator in rf_clf.estimators_:\n",
    "#     plt.figure(figsize=(12,12))\n",
    "#     tree.plot_tree(estimator, filled=True, rounded=True)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Looking for good `n_estimators`](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* GradientBoost\n",
    "    + an fixeds-size estimator normally has 8 to 32 leaves\n",
    "    + iteratively fit residuals by a split\n",
    "    + use learning rate to avoid high bias\n",
    "    + [Youtube Course](vhttps://www.youtube.com/watch?v=3CC4N4z3GJc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.initial_probs = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize probabilities for binary classification\n",
    "        self.initial_probs = np.full(y.shape, np.mean(y)) # y= np.array([1, 0, 0]) -> array([0.33333333, 0.33333333, 0.33333333])\n",
    "\n",
    "        # Convert to log-odds for starting point\n",
    "        initial_log_odds = np.log(self.initial_probs / (1 - self.initial_probs))\n",
    "        current_logits = np.full(y.shape, initial_log_odds)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals (negative gradients of log-loss)\n",
    "            probabilities = 1 / (1 + np.exp(-current_logits))\n",
    "            gradients = y - probabilities\n",
    "            \n",
    "            # Train a decision tree on the gradients\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            tree.fit(X, gradients)\n",
    "            self.models.append(tree)\n",
    "            \n",
    "            # Update the predictions\n",
    "            current_logits += self.learning_rate * tree.predict_proba(X)[:, 1]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        # Start with the initial probabilities\n",
    "        current_logits = np.full(X.shape[0], np.log(self.initial_probs / (1 - self.initial_probs)))\n",
    "        \n",
    "        # Add contributions from each trained tree\n",
    "        for tree in self.models:\n",
    "            current_logits += self.learning_rate * tree.predict_proba(X)[:, 1]\n",
    "        \n",
    "        # Convert back to probability using logistic function\n",
    "        probabilities = 1 / (1 + np.exp(-current_logits))\n",
    "        return np.vstack([1 - probabilities, probabilities]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Return class labels (0 or 1) based on predicted probabilities\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"pics/gradient-boost1.png\" width=\"500\"></center>\n",
    "\n",
    "<center><img src=\"pics/gradient-boost2.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.69\n",
      "The test error rate is 0.31\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.71      0.73      1197\n",
      "         1.0       0.60      0.66      0.63       803\n",
      "\n",
      "    accuracy                           0.69      2000\n",
      "   macro avg       0.68      0.68      0.68      2000\n",
      "weighted avg       0.69      0.69      0.69      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build a Gradient Boosting classifier with 200 trees\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.5, max_depth=1, random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('The accuracy on test data is {:.2f}'.format(gb_clf.score(X_test, y_test)))\n",
    "print('The test error rate is {:.2f}'.format(1 - gb_clf.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* AdaBoost\n",
    "<!-- \n",
    "    + an fix-sized estimator uses only one feature, i.e., one stump (one root node with two leaf nodes)\n",
    "    + weak learner\n",
    "    + build the subsequent stumps using the residuals\n",
    "    + the amount of say \n",
    "        $$\\alpha_m=\\log \\left(\\left(1-\\operatorname{err}_m\\right) / \\operatorname{err}_m\\right)$$\n",
    "\n",
    "    + Update the weights\n",
    "        $$w_i \\cdot \\exp \\left[\\alpha_m \\cdot I\\left(y_i \\neq G_m\\left(x_i\\right)\\right)\\right], i=1,2, \\ldots, N$$ -->\n",
    "<!-- Emphasize the need to correctly classify the examples with wrong predictions in the previous steps -->\n",
    "\n",
    "<!-- ```\n",
    "# build an Adaboost classifier with 200 trees\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5)\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(ada_clf.score(X_train, y_train)))\n",
    "print('The accuracy on test data is {:.2f}'.format(ada_clf.score(X_test, y_test)))\n",
    "print('The test error rate is {:.2f}'.format(1 - ada_clf.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "``` -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sit720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

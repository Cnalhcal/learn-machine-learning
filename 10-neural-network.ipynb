{"cells":[{"cell_type":"code","execution_count":75,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:21:42.665644Z","iopub.status.busy":"2023-05-17T12:21:42.664988Z","iopub.status.idle":"2023-05-17T12:21:52.301085Z","shell.execute_reply":"2023-05-17T12:21:52.299956Z","shell.execute_reply.started":"2023-05-17T12:21:42.665585Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from my_ml_package.classification import sigmoid\n","# import tensorflow as tf\n","# import torch\n","\n","# read titanic data\n","titanic_df = pd.read_csv('data/titanic_cleaned_data.csv')\n","\n","# define X and y\n","# Port of Embarkation: Q = Queenstown, S = Southampton\n","feature_cols = ['Pclass', 'Sex', 'Embarked_Q'] # , 'Embarked_S'\n","feature_idx2name = {i: col for i, col in enumerate(feature_cols)}\n","X = titanic_df[feature_cols]\n","y = titanic_df.Survived\n","X = X.to_numpy()\n","y = y.to_numpy()\n","Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=42)\n"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[{"data":{"text/plain":["(268, 3)"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["Xtest.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<!-- ## Intuitive Understanding of Neural Networks\n","* NN is analogous to the neural system of brains (**neural diagram** can be seen in the left of the figure below)\n","    + Brains rely on >100 billions neurons\n","    + Why is the model of brains so important for intelligence?  \n","        + communicate with each other via synapses. Each neuron can rececive up to 10K connections from/to other neurons; \n","        + \"Hundreds of different organs, [each carry out their own specific functions](https://human-memory.net/occipital-brain-lobe/)\". \n","        + occipital: process visual information; temporal: memory; frontal: planning; cerebellum: balance\n","    * Activation process: the process by which a neuron becomes active through connections with other neurons, which then stimulates (triggers the activation of) other connected neurons. \n","    + How to represent the neurons, their connections (synapses) and functional organs mathematically?\n","      + input/intermediate output values; Weights or paramters    \n","      + Each layer consists of **input/intermediate/output values (vertices/nodes/neurons)** connected by **weights** (synapses)\n","    + How does the activation process of a neural network stimulate different functions?\n","    forward pass contains different activation functoins and architectures -->"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","<!-- <center><img src=\"https://i.loli.net/2021/08/12/7yaGWTIKlHNz6mD.png\" width='500' ></center> -->\n","\n","* What are the differences between linear regression, logistic regression and neural network?\n","    + Logistic regression has no intermediate layer, while NN has multiple intermediate layers, each with multiple neurons. \n","    + i.e., a multivirate function with multiple inputs and one output vs a chain of multivariate functions with multiple inputs and multiple outputs -->\n","    \n","## Linear Function\n","* Notation: Let  $m$ = the number of examples, $n_i$ = the input dimension, $n_o$ = the output dimension\n","* The linear function:  map input $X$ to the output $y$ by finding the parameters $W$ (determined by loss function): \n","*  $\\hat{y} = W \\cdot X$  where $\\mathbf{X} \\in \\mathbb{R}^{n_i \\times m}$ and $\\mathbf{W} \\in \\mathbb{R}^{n_o \\times n_i}$ and $\\mathbf{\\hat{Y}} \\in \\mathbb{R}^{n_o \\times m}$\n","  \n","  \n","<!-- *  The form conforms to the implementation in Tensorflow： $\\hat{y} = X \\cdot W$  where $\\mathbf{X} \\in \\mathbb{R}^{m \\times n_i}$ and $\\mathbf{W} \\in \\mathbb{R}^{n_i \\times n_o}$ and $\\mathbf{\\hat{Y}} \\in \\mathbb{R}^{m \\times n_o}$  -->\n","Sure! Here is the same explanation with weight values replaced as variables:\n","\n","* Example of three neurons in a 2x3 linear layer where \\( n_i = 2 \\) and \\( n_o = 3 \\)\n","$$\\text{Neuron}_1 = w_{11} x_{1} + w_{12} x_{2}$$\n","$$\\text{Neuron}_2 = w_{21} x_{1} + w_{22} x_{2}$$\n","$$\\text{Neuron}_3 = w_{31} x_{1} + w_{32} x_{2}$$\n","\n","The output of this layer can be efficiently calculated via **matrix-vector product**:\n","$$\n","\\left[\\begin{array}{rr}\n","w_{11} & w_{12} & w_{13} \\\\\n","w_{21} & w_{22} & w_{23} \n","\\end{array}\\right]\\left[\\begin{array}{l}\n","x_1 \\\\\n","x_2 \\\\\n","x_3\n","\\end{array}\\right]\n","$$\n","\n","* We can efficiently calculate the values for \\( m \\) examples via matrix multiplication:\n","$$\n","\\left[\\begin{array}{rr}\n","w_{11} & w_{12} & w_{13} \\\\\n","w_{21} & w_{22} & w_{23} \n","\\end{array}\\right]\\left[\\begin{array}{llll}\n","x_1^1 & x_1^2 & \\ldots & x_1^m \\\\\n","x_2^1 & x_2^2 & \\ldots & x_2^m \\\\\n","x_3^1 & x_3^2 & \\ldots & x_3^m\n","\\end{array}\\right]\n","$$    \n","    \n","\n","<!--     + If a bias term $b\\in \\mathbb{R}^{n_o \\times 1}$ is added: $\\hat{y} = W \\cdot X + b$. But no need to do that since (1) $W$ do the job of transforming input or $b$ could be considered as  $w_{n+1} \\dot x$ where $x=1$.\n","    + Although the first notation makes more sense for me to imagine it as the transformation in the high-dimension space, the second notation is commonly used for implementation. Here I change (use $\\theta$ and transpose) the second notation into $\\theta^{T} X$. Because $\\theta$ which can avoid confusion for NLP notations and unify weights and bias.\n","    + Anyway, the linear node and bias nodes would not be shown in neural network diagrams since every neuron is assumed to have a linear node along with its corresponding bias. But we just use its output denoted as $z$ where $z = \\theta^{T} X$ -->\n","\n","<!-- * Function v.s. Equation: \n","    + each equation represents a line in two dimensional space (or a plane) \n","    + each function is an affine transformation (linear transformation with bias) which transform the whole plane e.g., $\\text{hidden_neuron_1} = 2 w_{1}- w_{2}$\n","\n","$$2 x_{1}- x_{2}=0$$\n","$$-x_{1} + 2x_{2}=3$$\n","This can be also represented in the matrix form to be solved efficiently.\n","$$\n","\\left[\\begin{array}{rr}\n","2 & -1 \\\\\n","-1 & 2\n","\\end{array}\\right]\\left[\\begin{array}{l}\n","x_1 \\\\\n","x_2 \n","\\end{array}\\right]=\\left[\\begin{array}{l}\n","0 \\\\\n","3\n","\\end{array}\\right]\n","$$ -->"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's make a one-layer neural network."]},{"cell_type":"code","execution_count":80,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T12:36:56.845559Z","iopub.status.busy":"2023-05-17T12:36:56.845133Z","iopub.status.idle":"2023-05-17T12:36:56.871924Z","shell.execute_reply":"2023-05-17T12:36:56.870970Z","shell.execute_reply.started":"2023-05-17T12:36:56.845525Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(2, 268)\n"]}],"source":["# 3-2 activations\n","W1 = np.random.randn(2, Xtest.shape[1])\n","b1 = np.random.randn(2, 1)\n","\n","z1 = np.dot(W1, Xtest.T) + b1\n","print(z1.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Activation Functions\n","What's the problem to nest linear transformation?\n","\n","<!-- > If each layer consisted only of an affine transformation, the entire function $f$ would also be affine -->\n","\n","\n","Add non-linearity into the neural network\n","* Sigmoid\n","    \n","    $$p =\\frac{1}{1+e^{-x}}$$\n","    + Note that the sigmoid function is also used for calculating probability in the last layer for computing the loss\n","* tanh\n","    $$p=\\frac{2}{1+e^{-2 x}}-1$$\n","\n","\n","* Thresholding Mechanism: Just like a biological neuron which fires only when the input signal is strong enough to cross a threshold, in a neural network, the activation function decides whether the node should get activated or not based on the input signal's strength. For example, the ReLU function activates a node only if the input is positive."]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["a1 = sigmoid(z1)"]},{"cell_type":"markdown","metadata":{},"source":["### Two Layers"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 268)\n","(268,)\n","0.585820895522388\n"]}],"source":["W2 = np.random.randn(1, 2)\n","b2 = np.random.randn(1, 1)\n","\n","z2 = np.dot(W2, a1) + b2\n","a2 = sigmoid(z2)\n","print(a2.shape)\n","yhat = (a2 > 0.5).astype(int).flatten()\n","print(yhat.shape)\n","accuracy = np.mean(yhat == ytest)\n","print(accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["## Building Multi-layers Neural Network"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of layers:  2\n"]}],"source":["def initialize_W_b(n_i, N_o):\n","    \"\"\" Initialize weights and biases for a neural network with n_i input neurons and N_o output neurons in each layer.\n","    Args:\n","        n_i: number of input neurons\n","        N_o: list of number of output neurons in each layer\n","    Returns:\n","        W: list of weight matrices, each with shape (n_o, n_i)\n","        b: list of bias vectors, eahc with shape (n_o, 1)\n","    \"\"\"\n","    W = []\n","    b = []\n","    for n_o in N_o:\n","        W.append(np.random.uniform(-1/np.sqrt(n_i), 1/np.sqrt(n_i), (n_o, n_i))) # shape (n_i, n_o)\n","        b.append(np.random.uniform(-1, 1, n_o).reshape(-1, 1)) # shape (n_o, 1)\n","        n_i = n_o\n","    return W, b\n","# 3-2-1 activations\n","N_o = [2,  1]\n","W_layers, b_layers = initialize_W_b(Xtest.shape[1], N_o )\n","print(\"Number of layers: \", len(W_layers))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Optimizing W, b\n","Recap: How to optimize weights of linear+sigmoid functions in logistic regression?\n","  <!-- * Optimization algorithm: [Gradient Descent](https://medium.com/@sergioli/understanding-gradient-descent-in-pytorch-dca50926bce4)\n","  * Use gradients to update weights\n","  * [Survey Paper from Sebastian Ruder](https://arxiv.org/abs/1609.04747) -->\n","  \n","What's the challenge for neural network?\n","<!-- * The calculation of gradients\n","* Backward propagation -->\n","\n","<!-- Combination of Layers/Architectures\n","* how model parameters are used by `torch.nn.Module`?\n","* how they **nest** different layers or operations?\n","* How does backward propagation work in pytorch `AutoGrad`? -->"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of layers:  2\n","0.7910447761194029\n"]}],"source":["N_o = [2,  1]\n","W_layers, b_layers = initialize_W_b(Xtest.shape[1], N_o )\n","print(\"Number of layers: \", len(W_layers))\n","\n","learning_rate = 0.001\n","for _ in range(1000):\n","    grad_W_layers, grad_b_layers = calculate_grads_for_all_layers( W_layers, b_layers, Xtrain, ytrain)\n","   \n","    # gradient descent\n","    W_layers[-1] -= learning_rate * grad_W_layers[-1]\n","    b_layers[-1] -= learning_rate * grad_b_layers[-1]\n","    W_layers[-2] -= learning_rate * grad_W_layers[-2]\n","    b_layers[-2] -= learning_rate * grad_b_layers[-2]\n","\n","# predict\n","activation = Xtest.T\n","for i in range(len(N_o)):\n","    z = np.dot(W_layers[i], activation) + b_layers[i]\n","    activation = sigmoid(z) # shape: (n_o, n_samples)\n","\n","yhat = (activation > 0.5).astype(int).flatten()\n","accuracy = np.mean(yhat == ytest)\n","print(accuracy)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["def compute_gradients_for_output_layer(last_activation, zs, y, a_prev):\n","    \"\"\"\n","    Compute the gradients of the loss with respect to the output layer's activation, z, weights, and biases.\n","    \n","    1. Compute Gradient of Loss w.r.t. Output (grad_al):\n","       - \\partial L / \\partial a_l: grad_al = (activation - ytrain) / (activation * (1 - activation))\n","    \n","    2. Compute Gradient of Loss w.r.t. z of the last layer (grad_zl):\n","       - \\partial L / \\partial z_l \n","         = \\partial L /  \\partial a_l \n","                       * \\partial a_l /  \\partial z_l \n","         = grad_al * sigmoid(zs[-1]) * (1 - sigmoid(zs[-1]))\n","    \n","    3. Compute Gradient of Loss w.r.t. Weights of the last layer (grad_Wl):\n","       - \\partial L / \\partial W_l \n","         = \\partial L /   \\partial z_l \n","                         *  \\partial z_l /  \\partial W_l \n","         = grad_zl @ activations[-2].T\n","    \"\"\"\n","    grad_al = (last_activation - y.reshape(1, -1)) / (last_activation * (1 - last_activation)) # shape: (n_o, n_samples)\n","    grad_zl = grad_al * sigmoid(zs[-1]) * (1 - sigmoid(zs[-1])) # shape: (n_o, n_samples)\n","    grad_Wl = grad_zl @ a_prev.T # shape: (n_o, n_i)\n","    grad_bl = grad_zl @ np.ones((last_activation.shape[1], 1)) # shape: (n_o, 1) \n","\n","    return grad_al, grad_zl, grad_Wl, grad_bl\n","\n","\n","def compute_gradients_for_linear_sigmoid(W_after, grad_z_after, z, a_prev):\n","    ############### Errors/delta #################\n","    # \\partial L /  \\partial a_{l-1} = \\partial L /   \\partial z_l\n","    #                                          * \\partial z_l /  \\partial a_{l-1}\n","    grad_a = W_after.T @ grad_z_after  # shape: (num_neurons, n_samples) \n","\n","    # \\partial L /  \\partial z_{l-1} = \\partial L /   \\partial a_{l-1}\n","    #                                          * \\partial a_{l-1} /  \\partial z_{l-1}\n","    grad_z = grad_a * (sigmoid(z) * (1 - sigmoid(z))) # shape: (num_neurons, n_samples)\n","\n","    ################# Gradients for W_{l-1} and b_{l-1} #################\n","    # \\partial L /  \\partial W_{l-1} = \\partial L /   \\partial z_{l-1}\n","    #                                          *  \\partial z_{l-1} /  \\partial W_{l-1}\n","    grad_W = grad_z @ a_prev.T # shape: (num_neurons, num_neurons_prev) \n","   \n","    # \\partial L /  \\partial b_{l-1} = \\partial L /   \\partial z_{l-1}\n","    #                                          *  \\partial z_{l-1} /  \\partial b_{l-1}\n","\n","    grad_b = grad_z @ np.ones((z.shape[1], 1)) # shape: (num_neurons, 1) = (num_neurons, n_samples) * (n_samples, 1)\n","\n","    return grad_a, grad_z, grad_W, grad_b\n","\n","\n","def calculate_grads_for_all_layers(W_layers, b_layers, Xtrain, ytrain):\n","    # init\n","    grads_W_layers = [np.zeros_like(W) for W in W_layers]\n","    grads_b_layers = [np.zeros_like(b) for b in b_layers]\n","\n","    # forward pass\n","    zs = []\n","    activations = []\n","    activations.append(Xtrain.T)\n","    activation = Xtrain.T \n","    for i in range(len(N_o)):\n","        z = np.dot(W_layers[i], activation) + b_layers[i]\n","        activation = sigmoid(z) # shape: (n_o, n_samples)\n","        zs.append(z)\n","        activations.append(activation)\n","    \n","    # backward pass: calculate gradients of the loss function w.r.t. W and b\n","    grad_al, grad_zl, grad_Wl, grad_bl = compute_gradients_for_output_layer(activations[-1], zs, ytrain, a_prev=activations[-2])\n","    grads_W_layers[-1] = grad_Wl\n","    grads_b_layers[-1] = grad_bl\n","\n","    for i in range(2, len(N_o)+1):\n","        assert W_layers[-(i-1)].shape[0] == grad_zl.shape[0]\n","        grad_a_prev, grad_z_prev, grad_W_prev, grad_b_prev = compute_gradients_for_linear_sigmoid(W_layers[-(i-1)], grad_zl, zs[-i], a_prev=activations[-(i+1)] )\n","        grads_W_layers[-i] = grad_W_prev\n","        grads_b_layers[-i] = grad_b_prev\n","        grad_zl = grad_z_prev\n","\n","    return grads_W_layers, grads_b_layers\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# # sklearn\n","# mlp = MLPClassifier(hidden_layer_sizes=(256,), \\\n","#                     activation=\"relu\", \\\n","#                     solver='adam',\\\n","#                     beta_1=0.9, beta_2=0.999, \\\n","#                     learning_rate='constant', learning_rate_init=0.001, \\\n","#                     max_iter=1000, random_state=0)\n","# for i in range(mlp.n_layers_)[:-1]:\n","#     print(i+1, '-th layer: ', mlp.coefs_[i].shape)\n","# mlp.fit(X_train, y_train)\n","# plt.plot(mlp.loss_curve_)\n","# plt.show()\n","# y_pred = mlp.predict(X_test)\n","# print(confusion_matrix(y_test, y_pred))\n","# print(classification_report(y_test, y_pred, target_names=target_names))"]},{"cell_type":"markdown","metadata":{},"source":["\n","In PyTorch\n","* The attribute `requires_grad` to control whether the gradients should be calculated during the backward. Specifically, after performing operations (e.g., sin) on the input tensor during forward propagation, the `grad_fn` attribute would be added into the output tensor in order to calculate gradients afterward.\n","\n","* Recording Tensor Operations: [The official introduction](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph) \n","* The `Tensor` class is the finest class we can trace the parameters and their corresponding gradients during backward propagation\n","> Conceptually, autograd keeps a record of data (tensors) & all executed\n","operations (along with the resulting new tensors) in a directed acyclic\n","graph (DAG) consisting of\n","[`Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n","objects. In this DAG, leaves are the input tensors, roots are the output\n","tensors. By tracing this graph from roots to leaves, you can\n","automatically compute the gradients using the chain rule.\n",">\n","> ![image.png](attachment:5e01a2e2-7506-448a-8e32-71531c5f6f12.png)![image.png](attachment:235839b6-fb87-45b4-a999-19d08bc69415.png)\n",">\n","> In a forward pass, autograd does two things simultaneously:\n","> \n","> - run the requested operation to compute a resulting tensor, and\n","> - maintain the operation’s *gradient function* in the DAG.\n",">\n","> The backward pass kicks off when ``.backward()`` is called on the DAG\n","root. ``autograd`` then:\n","> \n","> - computes the gradients from each ``.grad_fn``,\n","> - accumulates them in the respective tensor’s ``.grad`` attribute, and\n","> - using the chain rule, propagates all the way to the leaf tensors.\n","> \n","> Below is a visual representation of the DAG in our example. In the graph,\n","the arrows are in the direction of the forward pass. The nodes represent the backward functions\n","of each operation in the forward pass. The leaf nodes in blue represent our leaf tensors ``a`` and ``b``.\n","> \n","> .. figure:: /_static/img/dag_autograd.png\n","> \n","> <div class=\"alert alert-info\"><h4>Note</h4><p>**DAGs are dynamic in PyTorch**\n","  An important thing to note is that the graph is recreated from scratch; after each\n","  ``.backward()`` call, autograd starts populating a new graph. This is\n","  exactly what allows you to use control flow statements in your model;\n","  you can change the shape, size and operations at every iteration if\n","  needed.</p></div>\n","> \n","> Exclusion from the DAG\n",">\n","> ``torch.autograd`` tracks operations on all tensors which have their\n","``requires_grad`` flag set to ``True``. For tensors that don’t require\n","gradients, setting this attribute to ``False`` excludes it from the\n","gradient computation DAG.\n",">\n","> The output tensor of an operation will require gradients even if only a\n","single input tensor has ``requires_grad=True``."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.from_numpy(X_batch.T)\n","\n","\n","# create an input tensor\n","X_batch = X_train[:2]\n","y_batch = y_train[:2]\n","X_batch = torch.from_numpy(X_batch.T).float()\n","y_batch = torch.from_numpy(y_batch)\n","\n","W1 = np.random.uniform(-1/np.sqrt(n_i), 1/np.sqrt(n_i), (n_m, n_i))\n","W1 = torch.from_numpy(W1).float()\n","W1.requires_grad=True\n","A1= torch.matmul(W1, X_batch)\n","A1 = F.tanh(A1)\n","\n","W2 = np.random.uniform(-1/np.sqrt(n_m), 1/np.sqrt(n_m), (5, n_m))\n","W2 =  torch.from_numpy(W2).float()\n","W2.requires_grad=True\n","A2= torch.matmul(W2, A1)\n","y_prob = F.softmax(A2.T)\n","\n","# input tensor has no grad_fn\n","# print(X_batch.grad_fn)\n","\n","loss = F.nll_loss(torch.log(y_prob), y_batch)\n","\n","loss.backward()\n","\n","\n","\n","\n","# parameters\n","w = torch.Tensor([1.,2.,3.])\n","print('Original w grad:', w.grad)\n","\n","\n","# we create `x` containing two examples, each has 3-dimensional features\n","x1 = torch.Tensor([1., 2., 3.])\n","x2 = torch.Tensor([4., 5., 6.])\n","\n","y = x1 * w\n","z = y.sum()\n","z.backward()\n","print('w grad with x1:', w.grad)\n","\n","y = x2 * w\n","z = y.sum()\n","w.grad = None\n","z.backward()\n","print('w grad with x2:', w.grad)\n","\n","\n","# The gradients will be summed up in the batch dimension\n","x = torch.Tensor([[1., 2., 3.],\n","                  [4., 5., 6.]])\n","y = x * w\n","z = y.sum()\n","w.grad = None\n","z.backward()\n","print('mini-batch gradient of parameters with x1 and x2:', w.grad)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## More Complicated Neural Networks： convolution (CNN), recurrent (RNN), Transformer\n","\n","* Real-world data often contains properties that are sequential or spatial in nature, such as text, audio, video, sensor readings, and images. These properties can be important in determining the meaning of the data, and therefore need to be taken into account when processing the data.\n","\n","* Sequential properties refer to the fact that the positions of data points matter in explaining the overall meaning of the data, while spatial properties refer to the fact that the relative positions of data points matter in determining the meaning of the data. \n","* Communication using language: we use a sequence of words to express our thoughts and ideas. \n","    + The order in which we arrange these words is very important: \"The cat sat on the mat.\" v.s. \"The mat sat on the cat.\"\n","    + In order to accurately predict or classify language data, a neural network must be able to understand the relationships between words in a sentence and how they are ordered. Hence, sequential neural network architectures are designed to take into account the sequential nature of language data and can effectively learn the complex relationships between words in a sentence.\n","\n","<!-- * (Optional) The way I think of why we need different transformations in neural net: Just like playing Lego we need more than regular pieces🔺 to build real-world things 🚉, 📱... , neural network consist of more than linear transformation to model the real-world problem. This depends on prior knowledge of what problem you want to solve.  For example,  \n","if you build ⏰, you need irregularly shaped pieces like wheel🏽⚙️. \n","If you predict the increase of human population in China next year, you may use exponential function since you know  it will increase exponentially by time. \n","If you predict image class, you will use convolution transformation since yoou know image has to be understood by its combination of pixels rather than each pixel separately. -->"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"color:red\">I comment some codes for implementing different architectures here.</p>\n","<!-- ### 3.1 RNN\n","\n","More details see [RNN/LSTM](https://www.kaggle.com/sergioli212/rnn-lstm-from-scratch/)\n","\n","Code:\n","# input data with middle dimention 10 represents sequence length\n","X = torch.Tensor(np.random.randn(10, 3, 4))\n","n_x, m, T_x = X.shape # 3, 10, 4\n","n_y, n_a = 2, 5 \n","\n","# pytorch linear API + tanh activation function\n","linear_xa = torch.nn.Linear(n_x, n_a)\n","linear_aa = torch.nn.Linear(n_a, n_a)\n","a_prev = torch.randn(m, n_a)\n","for t in range(T_x):\n","    input1 = linear_xa(X[:,:,t])\n","    input2 = linear_aa(a_prev)\n","    a_prev = torch.tanh(input1+input2)\n","# print(a_prev.shape)\n","\n","\n","### 3.2 CNN\n","\n","# code\n","# keras\n","# tf.keras.layers.Conv1D(\n","#     filters,\n","#     kernel_size,\n","#     strides=1,\n","#     padding=\"valid\",\n","#     data_format=\"channels_last\",\n","#     dilation_rate=1,\n","#     groups=1,\n","#     activation=None,\n","#     use_bias=True,\n","#     kernel_initializer=\"glorot_uniform\",\n","#     bias_initializer=\"zeros\",\n","#     kernel_regularizer=None,\n","#     bias_regularizer=None,\n","#     activity_regularizer=None,\n","#     kernel_constraint=None,\n","#     bias_constraint=None,\n","#     **kwargs\n","# )\n","# The inputs are 128-length vectors with 10 timesteps, and the batch size is 4.  \n","input_shape = (4, 10, 128)\n","x = tf.random.normal(input_shape)\n","y = tf.keras.layers.Conv1D(32, 3, activation='relu',input_shape=input_shape[1:])(x)\n","print(y.shape)\n","\n","# pytorch: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d\n","# torch.nn.Conv1d(\n","#     in_channels,  # unlike keras, pytorch cannot infer the input channels since it has no placeholder for input\n","                    # put sequence length here\n","#     out_channels, \n","#     kernel_size, \n","#     stride=1,\n","#     padding=0, \n","#     dilation=1, \n","#     groups=1, \n","#     bias=True, \n","#     padding_mode='zeros')\n","x = tf.constant(\n","[[[1.,2.,3.],\n","  [2.,2.,1.],\n","  [3.,2.,6.]],\n","\n"," [[4.,4.,5.],\n","  [5.,5.,5.],\n","  [6.,6.,7.,],],\n","\n"," [[7.,8.,8.,],\n","  [8.,6.,7.,],\n","  [9.,8.,8.,]]] , dtype='float32')\n","print(x)\n","\n","# keras API\n","max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()\n","# max_pool_1d = tf.keras.layers.MaxPooling1D(3)\n","print(max_pool_1d(x))\n","\n","# jax API\n","from jax import random\n","import jax.numpy as jnp\n","key = random.PRNGKey(1701)\n","x = jnp.linspace(0, 10, 500)\n","y = jnp.sin(x) + 0.2 * random.normal(key, shape=(500,))\n","window = jnp.ones(10) / 10\n","y_smooth = jnp.convolve(y, window, mode='same')\n","plt.plot(x, y, 'lightgray')\n","plt.plot(x, y_smooth, 'black'); -->"]},{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-05-13T06:19:58.306836Z","iopub.status.busy":"2023-05-13T06:19:58.306127Z","iopub.status.idle":"2023-05-13T06:19:58.312114Z","shell.execute_reply":"2023-05-13T06:19:58.311094Z","shell.execute_reply.started":"2023-05-13T06:19:58.306793Z"}},"source":["\n","## 4. How to Acquire A Set of Weights Optimized for the Data/Task?\n","1. Randomly initialized weights\n","2. Define loss function, which can be used as an minimization objective function to optimize $W$.\n","3. Iteratively update weights to minimize the loss function using gradient descent\n","\n","### 4.1 [Loss function](https://www.kaggle.com/sergioli212/cross-entropy-loss-details).\n","* Intuitively, it is used to quantify, given empirical data (i.e., training samples), how wrong the current model predicts in contrast to the true values of samples. The quantity is called the loss between model outputs and grouth truth.\n","* Let's design loss function for classification and regression problems?\n","  \n","* The formal definition relates to **Bayes Theorem** for machine learning in Week 2 and **Minimizing Negative Log Likelhood** in Week 6\n","    + Bayes Theorem: maximize Pr(parameters|data) $\\propto$ Pr(data|parameters) * Pr(parameters)\n","    + Likelihood: $ L(\\theta) =  Pr((X, y)|\\theta) $\n","    + Given a set of $\\theta$, the likelihood can be calculated by \"the product of the probabilities for each data point\". The likelihood measures whether a given $\\theta$ is a good estimation. \n","$ L(\\theta) =  Pr((X, y)|\\theta)= \\prod_{i=1}^{N} Pr\\left(y^{(i)}, x^{(i)} | \\theta\\right) =Pr\\left(y^{(i)} | x^{(i)} , \\theta\\right)$\n","    + Why does it call likelihood not probability? \n","    <!-- ⚠️ although we fix $\\theta$ to caluculate the probability $Pr(X, y | \\theta)$, we never know the only true $\\theta$ to calculate the true 👉$Pr(X, y | \\theta)$. The probability is really just fake. That's why we give it a new name --- `LIKELIHOOD`.  -->\n","    + The optimal $\\theta$ should give the maximum liklihood or the minimum NLL $\\rightarrow$ We try our best to find a set of $\\theta$ which leads to NLL as low as possible, i.e., $ \\theta = argmax L(\\theta)=Pr(y | X , \\theta)$. \n","\n","<!--     We normally use Negative Log Likelhood (NLL). -->"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["MLE for Binary Classification\n","\n","* The last layer normaly has one neuron/probability from the sigmoid function, denoted as\n"," $ p=Pr(y = 1|x; \\theta)  $\n"," $$\\begin{aligned} NLL = -\\log L(\\theta) \\\\ &=-\\sum_{i=1}^{m} y^{(i)} \\log p+(1-y^{(i)}) \\log (1-p)) \\end{aligned}$$\n","\n","* It can be considered as **KL-devergence**, which measures the similarity between two distributions. [This post](https://jaketae.github.io/study/kl-mle/) proves that \"minimizing the KL divergence amounts to finding the maximum likelihood estimate of $\\theta$\". During the supervised learning，the ground-truth distribution is know. Hence, minizing **cross entropy** equals to the minimization of KL-devergence\n","$$D_{\\mathrm{KL}}(Y \\| \\hat{Y})=-\\sum_{i} y^{(\\mathrm{i})} \\log \\frac{\\hat{y}^{(\\mathrm{i})}}{y^{(\\mathrm{i})}}$$ "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_true = np.array([1, 0, 1])\n","y_prod = np.array([0.9, 0.1, 0.8])\n","\n","# calculate nll\n","log_probs = np.multiply(y_true ,np.log(y_prod)) + np.multiply((1-y_true), np.log(1-y_prod))\n","cost = (-1/len(y_true)) * np.sum(log_probs)\n","print(cost)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* The last layer normaly has one neuron/probability from the sigmoid function, denoted as\n"," $ p=Pr(y = 1|x; \\theta) $\n"," $$\\begin{aligned} NLL = -\\log L(\\theta) \\\\ &=-\\sum_{i=1}^{m} y^{(i)} \\log p+(1-y^{(i)}) \\log (1-p)) \\end{aligned}$$\n","\n","* It can be considered as **KL-devergence**, which measures the similarity between two distributions. [This post](https://jaketae.github.io/study/kl-mle/) proves that \"minimizing the KL divergence amounts to finding the maximum likelihood estimate of $\\theta$\". During the supervised learning，the ground-truth distribution is know. Hence, minizing **cross entropy** equals to the minimization of KL-devergence\n","$$D_{\\mathrm{KL}}(Y \\| \\hat{Y})=-\\sum_{i} y^{(\\mathrm{i})} \\log \\frac{\\hat{y}^{(\\mathrm{i})}}{y^{(\\mathrm{i})}}$$ "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["MLE for Multiclass Classification\n","\n","* Softmax Funtion: Except for sigmoid function, softmax function is used to calculate the output probability distribution for more than two classes\n","    $$p_i =\\frac{e^x_i}{\\sum_{i=1}^{K} e^{x_i}} $$"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["y_prob = None\n","y_true = None"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* How can we get y_prob? What should we change to the above neural net? \n","<!-- Softmax Funtion: Except for sigmoid function, softmax function is used to calculate the output probability distribution for more than two classes\n","    $$p_i =\\frac{e^x_i}{\\sum_{i=1}^{K} e^{x_i}} $$ -->\n","<!-- def softmax(z):\n","    e_z = np.exp(z)\n","    return e_z / np.sum(e_z, axis=-1).reshape(-1, 1)\n","\n","logits = [[4.0, 2.0, 1.0], \n","          [0.0, 5.0, 1.0]]\n","\n","y_prob = softmax(logits)\n","y_prob -->"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T11:37:46.821297Z","iopub.status.busy":"2023-05-17T11:37:46.820848Z","iopub.status.idle":"2023-05-17T11:37:46.832336Z","shell.execute_reply":"2023-05-17T11:37:46.831144Z","shell.execute_reply.started":"2023-05-17T11:37:46.821258Z"},"trusted":true},"outputs":[],"source":["# get y_prob from the neural network\n","\n","\n","# calculate nll"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-05-17T11:37:06.370766Z","iopub.status.busy":"2023-05-17T11:37:06.370125Z","iopub.status.idle":"2023-05-17T11:37:06.386798Z","shell.execute_reply":"2023-05-17T11:37:06.385457Z","shell.execute_reply.started":"2023-05-17T11:37:06.370719Z"},"trusted":true},"outputs":[],"source":["def softmax(z):\n","    e_z = np.exp(z)\n","    return e_z / np.sum(e_z, axis=-1).reshape(-1, 1)\n","\n","logits = [[4.0, 2.0, 1.0], \n","          [0.0, 5.0, 1.0]]\n","\n","\n","y_prob = softmax(logits)\n","y_prob\n","\n","\n","# one-hot representations\n","labels = np.array([[1],[2]])\n","\n","# L(y_pred, y) = -sum(y * log y_pred for each class) = -log y_pred[true_class_idx]\n","total_loss = -np.log(np.take_along_axis(y_prob, labels, axis=-1))\n","np.mean(total_loss)\n","\n","# pytorch API\n","loss = F.nll_loss(torch.from_numpy(np.log(y_prob)), labels))\n","print(loss)\n","# loss = F.cross_entropy(torch.tensor(logits), torch.tensor([1, 2]))\n","\n","# Tensorflow API\n","# tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Closing Words \n","* A neural network is an universal function approximator. For example, if you use softmax or sigmoid, a neural network represents conditional probability distribution. For understanding this, see [this notebook](https://www.kaggle.com/sergioli212/loss-function-and-optimization).\n","* the above models/functions are not trained so they are not a model fitting into any data. For understanding this, see [this notebook](https://www.kaggle.com/sergioli212/loss-function-and-optimization).\n","* Bullshit: The reason I would like to decompose NN architecture and modelling process is because, I think, I always see a lot of implementation packages and tutorials tend to mix up concepts or algorithms as a black box which hurts the flexibility to identify  and adjust finer pieces for solving my problem. For example, to generate adversarial text of NLP, I had a trouble to understand why we could update input instead of model parameters and what is problem of discrete text space and how to add constraints for ensuring fluency fo generating text via loss function (or adversarial objective functions here). All in all, each functions (linear, activation, stacking function, loss functions) during forward pass and their differentiation properties during backward pass should be well understood for deep learning practitioners.\n","\n","## Reference\n","* [How to initialize weights?](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":4}

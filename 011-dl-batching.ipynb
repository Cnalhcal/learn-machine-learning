{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def batch_generator(features, labels, batch_size):\n",
    "    total_samples = len(features)\n",
    "    indices = np.arange(total_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start in range(0, total_samples, batch_size):\n",
    "        end = min(start + batch_size, total_samples)\n",
    "        batch_indices = indices[start:end]\n",
    "        yield features[batch_indices], labels[batch_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading from Disk\n",
    "stream data from disk without loading the entire dataset into memory\n",
    "\n",
    "\n",
    "# Pipelining\n",
    " create efficient input pipelines by chaining multiple dataset transformations (e.g., map, batch, shuffle), "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing\n",
    "\n",
    "Parallelize data loading and preprocessing using functions like map, which can significantly speed up the training process.\n",
    "### `tf.data.Dataset`\n",
    "chain the `prefetch` method after the other data processing methods like map, batch, and shuffle to ensure that the data pipeline is optimized for parallel processing\n",
    "\n",
    "\n",
    "Yes, Hugging Face's `datasets` library provides similar functionality to TensorFlow's `tf.data.Dataset` but within the context of NLP tasks and the broader Hugging Face ecosystem. The `datasets` library allows you to handle and process large datasets efficiently. Here’s how you can achieve similar functionality with Hugging Face `datasets`:\n",
    "\n",
    "### Hugging Face Datasets\n",
    "\n",
    "1. **Loading Data:**\n",
    "   Hugging Face provides convenient methods to load various datasets.\n",
    "\n",
    "   ```python\n",
    "   from datasets import load_dataset\n",
    "\n",
    "   dataset = load_dataset('titanic')\n",
    "   ```\n",
    "\n",
    "2. **Mapping/Preprocessing:**\n",
    "   Use the `map` method to apply transformations to your dataset.\n",
    "\n",
    "   ```python\n",
    "   def preprocess_function(example):\n",
    "       # Apply preprocessing (e.g., normalization)\n",
    "       example['features'] = [x / 255.0 for x in example['features']]\n",
    "       return example\n",
    "\n",
    "   dataset = dataset.map(preprocess_function)\n",
    "   ```\n",
    "\n",
    "3. **Shuffling:**\n",
    "   You can shuffle the dataset to randomize the order of elements.\n",
    "\n",
    "   ```python\n",
    "   dataset = dataset.shuffle(seed=42)\n",
    "   ```\n",
    "\n",
    "4. **Batching:**\n",
    "   Hugging Face datasets do not have a direct `batch` method like TensorFlow. Instead, you typically handle batching within the data loader of your deep learning framework (e.g., PyTorch DataLoader).\n",
    "\n",
    "5. **Prefetching:**\n",
    "   While Hugging Face datasets do not have a specific `prefetch` method, they are designed to be efficient and can be used with data loaders that support prefetching (e.g., PyTorch’s `DataLoader` with the `num_workers` parameter).\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('csv', data_files='path/to/titanic.csv')\n",
    "\n",
    "# Example preprocessing function\n",
    "def preprocess_function(example):\n",
    "    # Assume 'features' is a list of feature values and 'label' is the target\n",
    "    example['features'] = [x / 255.0 for x in example['features']]\n",
    "    return example\n",
    "\n",
    "# Apply the preprocessing function\n",
    "dataset = dataset.map(preprocess_function)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Convert to PyTorch DataLoader\n",
    "def collate_fn(batch):\n",
    "    # Custom collate function to handle batching\n",
    "    features = [item['features'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "    return {'features': features, 'labels': labels}\n",
    "\n",
    "batch_size = 4\n",
    "dataloader = DataLoader(dataset['train'], batch_size=batch_size, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(\"Features:\")\n",
    "    print(batch['features'])\n",
    "    print(\"Labels:\")\n",
    "    print(batch['labels'])\n",
    "    print()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

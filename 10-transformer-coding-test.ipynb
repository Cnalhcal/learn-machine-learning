{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Coding Test \n",
    "\n",
    "You will be assessed overall on:\n",
    "\n",
    "    1) How far you get in the alloted time.\n",
    "    2) Code optimisations.\n",
    "    3) Code reusability.\n",
    "    4) Code readability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Coding Test\n",
    "Below you will find some code which is a classical example of the use of a transformer model.\n",
    "Your task is to complete the code such that the training runs and the validation loss decreases.\n",
    "\n",
    "Based on [Attention is all you need, Vaswani et al. 2017](https://arxiv.org/abs/1706.03762), write the code\n",
    "1. for the multi-head self-attention mechanism\n",
    "2. for the forward pass of the layer\n",
    "3. for generating causal masks.\n",
    "\n",
    "Finally, run the training and validation with the provided functions and dataset.\n",
    "\n",
    "Note: `pip install portalocker`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "\n",
    "import math\n",
    "from typing import Tuple, Optional\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "from torch.utils.data import dataset\n",
    "from torchtext.datasets import PennTreebank\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def attention(query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None, dropout = None) -> Tensor:\n",
    "    \"\"\" Computes the scaled dot-product attention over the provided query, key, and value tensors.\n",
    "\n",
    "    :param query: Tensor of shape (batch_size, n_heads, seq_len, dim), queries for attention.\n",
    "    :param key: Tensor of shape (batch_size, n_heads, seq_len, dim), keys for attention.\n",
    "    :param value: Tensor of shape (batch_size, n_heads, seq_len, dim), values for attention.\n",
    "    :param mask: Optional tensor for masking irrelevant parts of the input sequence.\n",
    "    :return: Tensor of shape (batch_size, n_heads, seq_len, dim), the weighted sum of the value using the attention weights.     \n",
    "    \"\"\"\n",
    "    # weights via scaled dot product attention\n",
    "    d_k = query.size(-1)\n",
    "    weights = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        \n",
    "        weights = weights.masked_fill(mask == 0, float('-inf')) # shape: (batch_size, n_heads, seq_len, seq_len)\n",
    "    normalized_weights = weights.softmax(dim=-1)\n",
    "\n",
    "    # apply dropout\n",
    "    if dropout is not None:\n",
    "        normalized_weights = dropout(normalized_weights)\n",
    "\n",
    "    return torch.matmul(normalized_weights, value)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, n_heads: int, dropout: Optional[float] = None):\n",
    "        \"\"\" Initializes the MultiHeadSelfAttention layer. \n",
    "\n",
    "        :param dim: Total dimensionality of the input and output of the self-attention layer.\n",
    "        :param n_heads: Number of attention heads. `dim` should be divisible by `n_heads` to equally\n",
    "                        split the dimensionality across all heads.\n",
    "        :param dropout: Optional dropout rate for attention weights; defaults to None if not provided.\n",
    "        \"\"\"\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = dim // n_heads\n",
    "        assert n_heads * self.head_dim == dim, f\"embedding dim={dim} not divisible by n_heads={n_heads}.\"\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear_query = nn.Linear(dim, dim)\n",
    "        self.linear_key = nn.Linear(dim, dim)\n",
    "        self.linear_value = nn.Linear(dim, dim)\n",
    "        self.linear_cat_attn = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\" The forward method for MultiHeadSelfAttention.\n",
    "        \n",
    "        :param query: Tensor of shape (batch_size, seq_len, dim), queries for attention.\n",
    "        :param key: Tensor of shape (batch_size, seq_len, dim), keys for attention.\n",
    "        :param value: Tensor of shape (batch_size, seq_len, dim), values for attention.\n",
    "        :param mask: Optional tensor for masking irrelevant parts of the input sequence.\n",
    "        :return: Tensor of shape (batch_size, seq_len, dim), the output of the attention layer.\n",
    "        \"\"\"\n",
    "        # linear projection\n",
    "        query = self.linear_query(query)\n",
    "        key = self.linear_key(key)\n",
    "        value = self.linear_value(value)\n",
    "\n",
    "        # split the heads\n",
    "        query = query.view(query.shape[0], -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        key = key.view(key.shape[0], -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(value.shape[0], -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # attention output: (batch_size, n_heads, seq_len, dim)\n",
    "        attn_output =  attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(query.size(0), -1, self.dim)\n",
    "\n",
    "        return self.linear_cat_attn(attn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, attn: MultiHeadSelfAttention, d_model: int, dim_feedforward: int, layer_norm_eps=1e-5, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.attn = attn\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\" The encoder block, generally, passes the input through a multi-head self-attention layer followed by feedforward networks to transform the input.\n",
    "    \n",
    "\n",
    "        :param x: Tensor of shape (batch_size, seq_len, d_model), the input to the encoder block.\n",
    "        :param mask: Optional tensor of shape (batch_size, seq_len, seq_len), a mask tensor for the\n",
    "                     self-attention layer to ignore certain positions within the input sequence.\n",
    "        :return: Tensor of shape (batch_size, seq_len, d_model), the output of the encoder block.\n",
    "        \"\"\"\n",
    "        # a multi-head self-attention layer -> residual connection -> normalization\n",
    "        x = self.norm1(x + self._self_attn(x, mask))\n",
    "\n",
    "        # feedforward network -> residual connection -> normalization\n",
    "        x = self.norm2(x + self._feed_forward(x))\n",
    "        return x\n",
    "\n",
    "    def _self_attn(self, x: Tensor, mask: Optional[Tensor]) -> Tensor:\n",
    "        x = self.attn(x, x, x, mask)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _feed_forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.dropout(self.activation(self.linear1(x)))\n",
    "        return self.dropout2(self.linear2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, encoder: EncoderBlock, n_blocks: int):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoder_blocks = nn.ModuleList([deepcopy(encoder) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x: Tensor, mask: Optional[Tensor] = None) -> Tensor:\n",
    "        for encoder in self.encoder_blocks:\n",
    "            x = encoder(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEncodingTorch(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class TransformerModelManualAttn(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.pos_encoder = PositionalEncodingTorch(d_model, dropout)\n",
    "        encoder_layers = EncoderBlock(attn=MultiHeadSelfAttention(dim=d_model, n_heads=nhead, dropout=dropout),\n",
    "                                      d_model=d_model, dim_feedforward=d_hid, dropout=dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Move the input data through the transformer model. The process involves\n",
    "        embedding the input, applying positional encoding, passing the result through\n",
    "        a transformer encoder, and finally projecting the output to the vocabulary space.\n",
    "\n",
    "        :param src: Tensor of shape (batch_size, seq_len), containing the indices of input tokens.\n",
    "        :param src_mask: Optional tensor used for masking in self-attention layers.\n",
    "        :return: Tensor of shape (batch_size, seq_len, ntoken), containing the logits of predicted token probabilities.\n",
    "        \"\"\"\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in training data: 924412\n",
      "Number of tokens in validation data: 73339\n",
      "Number of tokens in test data: 82114\n"
     ]
    }
   ],
   "source": [
    "# Load data, load tokenizer, build vocabulary\n",
    "train_iter, val_iter, test_iter = PennTreebank()\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# get train, validation, and test data\n",
    "train_data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in train_iter] # shape (num_examples, num_tokens)\n",
    "val_data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in val_iter]\n",
    "test_data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in test_iter] \n",
    "\n",
    "# concat into one long sequence for training, validation, and testing\n",
    "train_data = torch.cat(tuple(filter(lambda t: t.numel() > 0, train_data))) # shape (num_tokens_in_text1+num_tokens_in_text2+num_tokens_in_text3+...,)\n",
    "val_data = torch.cat(tuple(filter(lambda t: t.numel() > 0, val_data)))\n",
    "test_data = torch.cat(tuple(filter(lambda t: t.numel() > 0, test_data)))\n",
    "\n",
    "# print the lengths of the training, validation, and test data\n",
    "print(f\"Number of tokens in training data: {train_data.size(0)}\")\n",
    "print(f\"Number of tokens in validation data: {val_data.size(0)}\")\n",
    "print(f\"Number of tokens in test data: {test_data.size(0)}\")\n",
    "\n",
    "# Number of tokens in training data: 924412\n",
    "# Number of tokens in validation data: 73339\n",
    "# Number of tokens in test data: 82114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequence length after trimming and batching: 46220\n",
      "Validation sequence length after trimming and batching: 7333\n",
      "Test sequence length after trimming and batching: 8211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[  99,  244,  291,  ...,    2,  933,    5],\n",
       "        [  15,    4,   71,  ...,    2,  428,   61],\n",
       "        [  25,   44,  124,  ...,    1, 3625,   48],\n",
       "        ...,\n",
       "        [ 188, 5267,    0,  ...,    0,    0,   95],\n",
       "        [   5,    4,    3,  ...,    7, 1230,   65],\n",
       "        [ 300,  434,    1,  ...,   34,   12,  380]], device='mps:0')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# batchify the data\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "\n",
    "# sequence lengths\n",
    "train_seq_len = train_data.shape[0] // batch_size\n",
    "val_seq_len = val_data.shape[0] // eval_batch_size\n",
    "test_seq_len = test_data.shape[0] // eval_batch_size\n",
    "\n",
    "# trim the data to fit the batch size\n",
    "train_data = train_data[:train_seq_len * batch_size]\n",
    "val_data = val_data[:val_seq_len * eval_batch_size]\n",
    "test_data = test_data[:test_seq_len * eval_batch_size]\n",
    "\n",
    "# reshape the data where each column is a sequence\n",
    "train_data = train_data.view(batch_size, train_seq_len).t().contiguous()\n",
    "val_data = val_data.view(eval_batch_size, val_seq_len).t().contiguous()\n",
    "test_data = test_data.view(eval_batch_size, test_seq_len).t().contiguous()\n",
    "\n",
    "print(\"Training sequence length after trimming and batching:\", train_data.size(0))\n",
    "print(\"Validation sequence length after trimming and batching:\", val_data.size(0))\n",
    "print(\"Test sequence length after trimming and batching:\", test_data.size(0))\n",
    "# Training sequence length after trimming and batching: 46220\n",
    "# Validation sequence length after trimming and batching: 7333\n",
    "# Test sequence length after trimming and batching: 8211\n",
    "\n",
    "\n",
    "# move to the device\n",
    "device = \"mps\" # torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "train_data.to(device)\n",
    "val_data.to(device)\n",
    "test_data.to(device)\n",
    "\n",
    "# ======================  My local Python environment does not have the required packages to run this code. Due to time constraint, \n",
    "# ======================= I cannot re-set a new one to run this code. I am sorry for the inconvenience for reviewing my code. =======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Parameters\n",
    "ntokens = len(vocab)\n",
    "emsize = 200\n",
    "d_hid = 200\n",
    "nlayers = 2\n",
    "nhead = 2\n",
    "dropout = 0.1\n",
    "use_causal_mask = True\n",
    "\n",
    "# Create Model\n",
    "model = TransformerModelManualAttn(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training --- One Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20])\n"
     ]
    }
   ],
   "source": [
    "window_size = 35\n",
    "start_seq_pos = 0\n",
    "batch_win1 = train_data[start_seq_pos:start_seq_pos + window_size] \n",
    "print(batch_win1.shape) # shape: (window_size, batch_size)\n",
    "# torch.Size([35, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 20])\n"
     ]
    }
   ],
   "source": [
    "target_win1 = train_data[start_seq_pos + 1:start_seq_pos + window_size + 1] # shape (window_size, batch_size)\n",
    "print(target_win1.shape)  # shape: (window_size, batch_size)\n",
    "# torch.Size([35, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 35, 35])\n"
     ]
    }
   ],
   "source": [
    "# mask\n",
    "mask = torch.tril(torch.ones((1, window_size, window_size)))\n",
    "\n",
    "print(mask.shape) # shape: (1, window_size, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_win1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:  torch.Size([20, 2, 35, 35])\n",
      "Key:  torch.Size([20, 2, 35, 100])\n",
      "Weights:  torch.Size([20, 2, 35, 35])\n",
      "Key:  torch.Size([20, 2, 35, 100])\n"
     ]
    }
   ],
   "source": [
    "batch_win1 = batch_win1.to(device)\n",
    "mask = mask.to(device) \n",
    "\n",
    "output = model(src=batch_win1.T, src_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 6.4839e-01, -1.9069e-01, -6.7224e-01,  ...,  1.4212e+00,\n",
       "           6.8141e-02, -3.2933e-01],\n",
       "         [-8.3399e-01, -5.4409e-01, -1.0539e+00,  ...,  1.3813e+00,\n",
       "          -2.3350e-01, -2.6184e-01],\n",
       "         [ 1.2623e-01, -1.8705e-01, -3.3221e-01,  ...,  1.3431e+00,\n",
       "          -7.6831e-01, -3.7534e-01],\n",
       "         ...,\n",
       "         [-4.2288e-01, -3.4111e-01, -8.1829e-01,  ...,  1.6811e+00,\n",
       "          -2.0424e-01, -6.1760e-01],\n",
       "         [-1.2379e-01, -4.9680e-01, -6.8506e-01,  ...,  1.7153e+00,\n",
       "          -3.4596e-01, -9.7069e-01],\n",
       "         [ 1.6114e-01, -4.3917e-01, -5.6082e-01,  ...,  2.0197e+00,\n",
       "          -6.7841e-01, -4.9689e-01]],\n",
       "\n",
       "        [[-1.1922e+00, -1.1030e-01, -1.6944e-01,  ...,  3.0151e-01,\n",
       "           2.7070e-01, -6.6524e-01],\n",
       "         [-1.1228e+00, -4.6843e-01, -4.3408e-02,  ...,  8.9467e-01,\n",
       "          -9.1702e-01,  6.2178e-03],\n",
       "         [-1.5348e+00, -3.3282e-01, -1.4233e-01,  ...,  6.1153e-01,\n",
       "          -6.1223e-01, -4.2305e-01],\n",
       "         ...,\n",
       "         [-5.6631e-01,  4.0741e-01, -2.7523e-01,  ...,  1.0222e+00,\n",
       "          -3.6688e-01,  2.3992e-01],\n",
       "         [-1.0675e+00, -4.3217e-02, -1.8944e-01,  ...,  8.1678e-01,\n",
       "          -7.6116e-01, -7.3973e-01],\n",
       "         [-1.1915e+00, -1.2287e+00, -4.0984e-01,  ...,  9.1846e-01,\n",
       "           2.5470e-01, -1.8725e-01]],\n",
       "\n",
       "        [[-1.4622e+00, -6.3954e-01,  1.8231e-01,  ...,  3.3869e-01,\n",
       "          -2.3788e-01, -4.9591e-01],\n",
       "         [-9.1123e-01, -5.7186e-01,  5.0334e-02,  ...,  7.0986e-01,\n",
       "          -6.9605e-01, -4.5707e-01],\n",
       "         [-1.8943e+00, -1.2047e+00, -7.8321e-02,  ...,  2.7388e-01,\n",
       "           7.1165e-01, -3.2120e-01],\n",
       "         ...,\n",
       "         [-1.3940e+00,  4.5191e-02, -3.3061e-01,  ...,  4.0996e-01,\n",
       "          -7.8564e-01, -6.8969e-02],\n",
       "         [-9.4669e-01,  2.1400e-02,  2.7793e-01,  ...,  2.6860e-01,\n",
       "          -4.4827e-01, -3.0970e-01],\n",
       "         [-1.3974e+00, -1.7907e-01, -1.2546e-01,  ...,  1.8088e-01,\n",
       "          -6.0257e-01,  4.8706e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 5.5508e-01, -2.8683e-02, -5.8296e-01,  ..., -5.0905e-02,\n",
       "           1.0358e-01,  3.9304e-01],\n",
       "         [ 7.8377e-01, -3.2499e-01, -2.3538e-01,  ..., -1.6597e-01,\n",
       "           3.9666e-01, -2.0726e-02],\n",
       "         [ 1.4697e+00, -2.0991e-01,  5.0941e-01,  ...,  1.7009e-01,\n",
       "          -5.3282e-01, -4.4537e-04],\n",
       "         ...,\n",
       "         [ 1.2803e+00, -4.2193e-01,  6.1209e-03,  ...,  3.4718e-01,\n",
       "           1.2791e-02, -6.6782e-01],\n",
       "         [ 8.0284e-01, -4.8784e-01, -7.6824e-03,  ...,  2.6631e-01,\n",
       "          -5.9304e-01, -2.5881e-01],\n",
       "         [ 1.1666e+00, -4.4616e-01, -6.8111e-01,  ...,  1.2115e+00,\n",
       "           1.1944e-01, -3.4828e-01]],\n",
       "\n",
       "        [[ 7.5893e-01, -3.9025e-01, -7.2921e-01,  ...,  3.6934e-01,\n",
       "           9.6047e-02,  2.0140e-02],\n",
       "         [ 4.7300e-01, -8.1199e-01, -7.7366e-01,  ...,  5.2802e-01,\n",
       "           4.4245e-01, -8.2099e-02],\n",
       "         [ 8.3696e-01, -3.4311e-01, -1.8845e-01,  ..., -2.2772e-01,\n",
       "          -1.6078e-03,  7.6293e-02],\n",
       "         ...,\n",
       "         [ 1.0836e+00, -2.4481e-02, -2.3874e-01,  ...,  3.3802e-01,\n",
       "          -8.7118e-01,  3.6481e-01],\n",
       "         [ 1.1356e+00, -2.0222e-01, -3.5314e-01,  ..., -3.0115e-02,\n",
       "          -1.8580e-01,  4.6539e-01],\n",
       "         [ 7.4560e-01, -6.9731e-01, -5.0570e-01,  ...,  1.9678e-01,\n",
       "          -3.7400e-01, -4.9194e-01]],\n",
       "\n",
       "        [[ 1.3982e+00, -3.2738e-01, -3.8924e-01,  ..., -4.5366e-01,\n",
       "           3.4806e-02,  3.9468e-01],\n",
       "         [ 1.0157e+00, -7.7063e-01,  3.4793e-01,  ...,  1.2868e-01,\n",
       "          -2.9017e-01,  7.8335e-01],\n",
       "         [ 4.7358e-01,  1.8427e-01,  2.3339e-01,  ..., -2.6436e-01,\n",
       "          -3.9806e-01,  1.0085e+00],\n",
       "         ...,\n",
       "         [ 6.8873e-01, -9.2569e-02, -4.2321e-01,  ...,  9.8785e-02,\n",
       "           2.6897e-01,  9.8677e-01],\n",
       "         [ 6.3815e-01, -5.3258e-01, -8.6997e-02,  ..., -4.5426e-01,\n",
       "           2.5317e-01,  8.2796e-01],\n",
       "         [ 5.0367e-01, -6.4222e-01, -6.8035e-01,  ...,  2.3974e-01,\n",
       "          -3.9040e-01,  8.5019e-01]]], device='mps:0',\n",
       "       grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([700, 9922])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.view(-1, ntokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output.view(-1, ntokens), target_win1.to(device).reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & Eval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source: Tensor, i: int, window_size: int) -> Tuple[Tensor, Tensor]:\n",
    "    seq_len = min(window_size, len(source) - 1 - i) # ensure that each token in the batch has a next token\n",
    "    data = source[i:i + seq_len]\n",
    "    target = source[i + 1:i + seq_len + 1].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "def train(\n",
    "        model,\n",
    "        train_data: Tensor,\n",
    "        window_size: int,\n",
    "        criterion,\n",
    "        ntokens: int,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        epoch: int = 0,\n",
    "        device: torch.device = None,\n",
    "        use_causal_mask: bool = True\n",
    ") -> None:\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = torch.tril(torch.ones((1, window_size, window_size))).to(device) if use_causal_mask else None\n",
    "    num_batches = len(train_data) // window_size\n",
    "    for batch, i in enumerate(range(0, train_data.shape[0] - 1, window_size)):  # [0, 35, 70, 105, ...]\n",
    "        data, targets = get_batch(train_data, i, window_size=window_size)\n",
    "        if data.shape[0] < window_size and src_mask is not None:\n",
    "            src_mask = torch.tril(torch.ones((1, data.shape[0], data.shape[0]))).to(device)\n",
    "        output = model(src=data, src_mask=src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f}'\n",
    "                  f' | ppl {ppl:8.2f}'\n",
    "                  )\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "        model,\n",
    "        eval_data: Tensor,\n",
    "        window_size: int,\n",
    "        ntokens: int,\n",
    "        criterion,\n",
    "        device: torch.device = None,\n",
    "        use_causal_mask: bool = True,\n",
    ") -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    src_mask = torch.tril(torch.ones((1, window_size, window_size))).to(device) if use_causal_mask else None\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.shape[0] - 1, window_size):\n",
    "            data, targets = get_batch(eval_data, i, window_size=window_size)\n",
    "            if data.shape[0] < window_size and src_mask is not None:\n",
    "                src_mask = torch.tril(torch.ones((1, data.shape[0], data.shape[0]))).to(device)\n",
    "            output = model(data, src_mask=src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += data.shape[0] * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)\n",
    "\n",
    "bptt = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, )\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 10\n",
    "\n",
    "# Train\n",
    "home = os.path.join(os.path.expanduser(\"~\"), \"transformer_test\")\n",
    "save_dir = os.path.join(os.path.join(home, 'pytorch_example')) # feel free to change path\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "best_model_params_path = os.path.join(save_dir, \"best_model_params.pt\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_data, bptt, criterion, ntokens, optimizer, scheduler, epoch, device, use_causal_mask)\n",
    "    val_loss = evaluate(model, val_data, bptt, ntokens, criterion, device, use_causal_mask)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    scheduler.step()\n",
    "model.load_state_dict(torch.load(best_model_params_path))  # load best model states\n",
    "\n",
    "# Test\n",
    "test_loss = evaluate(model, test_data, bptt, ntokens, criterion, device)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. General knowledge and LLMs\n",
    "\n",
    "**Describe the attention complexity in memory and computation costs. Do you know methods to try to reduce this cost?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Computational Cost is in the complexity of $O(N^2 \\dot D)$ if each query/key is of dimension $D$ and there are $N$ tokens\n",
    "* The dot product operation for a single pair of query and key is $O(D)$.\n",
    "* This dot product is computed for each pair of tokens in a sequence of length $N$.\n",
    "\n",
    "The memory cost is in the complexity of $O(N^2 \\dot D)$ because the attention matrix (N by N) needs to be stored during the forward pass to calculate gradients during backpropagation\n",
    "\n",
    "Methods to reduce the cost:\n",
    "* Sparse Attention \n",
    "* Low-Rank Approximations (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Why use a causal mask in attention? Are there any other interesting masks or patterns that can be used for training or for generation?**\n",
    "* At the training timen, a causal mask for auto-regressive language modeling can prevent the model from \"seeing the future tokens.\n",
    "* Padding Mask is commonly used to ignore padding tokens in input sequences, ensuring that these don't affect the computation of attention scores. This is commonly used for many NLP tasks, e.g., auto-encoding language modeling and text classification via batching. If batched generation is required, it can be used for generative tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

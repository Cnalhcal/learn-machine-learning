{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# A Toy Example\n",
    "param = tf.Variable([1.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = (param - 3) ** 2\n",
    "grad = tape.gradient(loss, param)\n",
    "print(\"Gradient before step:\", grad)\n",
    "w  = tf.Variable([1.0, 2.0])\n",
    "x  = tf.Variable([4.0, 6.0])\n",
    "with tf.GradientTape() as tape:\n",
    "  y = w * x\n",
    "tape.gradient(y, w)\n",
    "\n",
    "\n",
    "\n",
    "# Normally, the input sample has more than 1 example. Let's take `num_example=2`. \n",
    "num_example = 2\n",
    "tf.random.set_seed(22)\n",
    "W = tf.Variable(tf.random.normal((2, 10)))\n",
    "X = tf.constant(tf.random.normal((10, num_example)))\n",
    "print('W:', W.numpy())\n",
    "print('X:', X.numpy())\n",
    "\n",
    " \n",
    "with  tf.GradientTape() as tape:\n",
    "    z = tf.matmul(W, X)\n",
    "    print('z:', z.numpy())\n",
    "\n",
    "dz_dW = tape.gradient(z, W)\n",
    "dz_dW\n",
    "\n",
    "# The gradients w.r.t `W` we got are summed from two examples. \n",
    "# For example, since the gradient w.r.t w1 or `W[0]` equals to x1 or `X[0]`, \n",
    "# here we got 0.2138209(= -0.12328745+0.33710834). \n",
    "# However, during the backward pass of neural network, \n",
    "# we average the gradients across training examples(considering the learning rate, it doesnot necessarily need to be averaged). \n",
    "# Some common misunderstanding may be found [here](https://datascience.stackexchange.com/questions/33489/why-averaging-the-gradient-works-in-gradient-descent).\n",
    "# However, if the function output is a vector, it would compute the gradients of vector's sum. \n",
    "# Check [`jacobian`](https://www.tensorflow.org/api_docs/python/tf/GradientTape#jacobian) for computing autodiff for each element in the output vector.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

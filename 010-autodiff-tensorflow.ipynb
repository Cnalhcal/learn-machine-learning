{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = tf.Variable([1.0])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = (param - 3) ** 2\n",
    "grad = tape.gradient(loss, param)\n",
    "print(\"Gradient before step:\", grad)\n",
    "w  = tf.Variable([1.0, 2.0])\n",
    "x  = tf.Variable([4.0, 6.0])\n",
    "with tf.GradientTape() as tape:\n",
    "  y = w * x\n",
    "tape.gradient(y, w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [[-0.7258272   1.0123503  -0.7519852  -0.88858026 -2.0784743   0.13739629\n",
      "   0.24744615 -0.42350662 -1.5114709  -0.39976093]\n",
      " [-0.6800961  -0.11622717 -0.24688219 -0.40650508  0.7409944   1.0018212\n",
      "  -0.00657086 -1.2199409  -0.8227369  -2.096853  ]]\n",
      "X: [[-0.12328736  0.33710814]\n",
      " [-0.15698127  0.51004434]\n",
      " [ 0.77993774 -0.12912273]\n",
      " [ 0.28278875  1.2848064 ]\n",
      " [-0.3107411   1.5029539 ]\n",
      " [-0.35227138 -0.80474746]\n",
      " [-0.30448258 -0.9182585 ]\n",
      " [-0.27900812  1.9815253 ]\n",
      " [-0.14969558  1.0174019 ]\n",
      " [-1.9002974  -0.3961953 ]]\n",
      "z: [[ 0.7189936 -6.453113 ]\n",
      " [ 3.6615932 -2.8890827]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.21382079,  0.35306305,  0.650815  ,  1.5675951 ,  1.1922128 ,\n",
       "        -1.1570189 , -1.2227411 ,  1.7025172 ,  0.86770636, -2.2964926 ],\n",
       "       [ 0.21382079,  0.35306305,  0.650815  ,  1.5675951 ,  1.1922128 ,\n",
       "        -1.1570189 , -1.2227411 ,  1.7025172 ,  0.86770636, -2.2964926 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Normally, the input sample has more than 1 example. Let's take `num_example=2`. \n",
    "num_example = 2\n",
    "tf.random.set_seed(22)\n",
    "W = tf.Variable(tf.random.normal((2, 10)))\n",
    "X = tf.constant(tf.random.normal((10, num_example)))\n",
    "print('W:', W.numpy())\n",
    "print('X:', X.numpy())\n",
    "\n",
    " \n",
    "with  tf.GradientTape() as tape:\n",
    "    z = tf.matmul(W, X)\n",
    "    print('z:', z.numpy())\n",
    "\n",
    "dz_dW = tape.gradient(z, W)\n",
    "dz_dW\n",
    "\n",
    "# The gradients w.r.t `W` we got are summed from two examples. \n",
    "# For example, since the gradient w.r.t w1 or `W[0]` equals to x1 or `X[0]`, \n",
    "# here we got 0.2138209(= -0.12328745+0.33710834). \n",
    "# However, during the backward pass of neural network, \n",
    "# we average the gradients across training examples(considering the learning rate, it doesnot necessarily need to be averaged). \n",
    "# Some common misunderstanding may be found [here](https://datascience.stackexchange.com/questions/33489/why-averaging-the-gradient-works-in-gradient-descent).\n",
    "# However, if the function output is a vector, it would compute the gradients of vector's sum. \n",
    "# Check [`jacobian`](https://www.tensorflow.org/api_docs/python/tf/GradientTape#jacobian) for computing autodiff for each element in the output vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(25,) dtype=float64, numpy=\n",
      "array([-0.3473799 ,  0.49434512,  0.77298346,  1.08673934,  0.69588929,\n",
      "        0.68267483,  0.29298751, -0.61848185,  0.01832627,  0.06835366,\n",
      "        0.62955296, -0.39228602,  0.45002156,  0.23983424, -0.99758659,\n",
      "       -0.21402083, -0.18942877,  1.3108772 , -2.02840606, -0.10437081,\n",
      "       -1.2195235 ,  0.24866279,  1.31752354,  0.217997  ,  0.83953719])>\n",
      "tf.Tensor(\n",
      "[ 1.3052402   2.98869023  3.54596692  4.17347868  3.39177859  3.36534966\n",
      "  2.58597501  0.7630363   2.03665253  2.13670732  3.25910592  1.21542797\n",
      "  2.90004312  2.47966848  0.00482683  1.57195835  1.62114246  4.6217544\n",
      " -2.05681213  1.79125839 -0.439047    2.49732557  4.63504709  2.43599401\n",
      "  3.67907437], shape=(25,), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "v = np.random.randn(25)\n",
    "v = tf.Variable(v)\n",
    "print(v)\n",
    "with tf.GradientTape() as tape:\n",
    "    out =  (v+1) ** 2\n",
    "grad = tape.gradient(out, v)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

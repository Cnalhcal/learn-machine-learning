{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from my_ml_package.visualize import plot_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the problem of the accuracy?\n",
    "* Use the patient/sample-and-doctor/model analogy to understand why we need both other evaluation metrics for error analysis\n",
    "* in which class of sample misclassification happen?\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred1: 1.0\n",
      "Pred2: 0.75\n"
     ]
    }
   ],
   "source": [
    "# 1 represents the with-cancer class; 0 represents the without-cancer class\n",
    "y =      [0, 0, 0, 0, 1, 1, 1, 1]\n",
    "\n",
    "# the first two predictions are FP\n",
    "y_pred1 = [1, 1, 0, 0, 1, 1, 1, 1] # 2 non cancer -> cancer\n",
    "# FP       *  * \n",
    "# 4/4 = 100%\n",
    "print('Pred1:', metrics.recall_score(y, y_pred1))\n",
    "\n",
    "\n",
    "\n",
    "# the first two predictions are FN\n",
    "y_pred2 = [0, 0, 0, 0, 1, 0, 1, 1]   # 1 cancer ->  no cancer\n",
    "# FN  \n",
    "print('Pred2:', metrics.recall_score(y, y_pred2))\n",
    "# 3/4 = 75%                    *\n",
    "\n",
    "#               pred0     pred1\n",
    "# actual 0       [[TN,      ],\n",
    "# actual 1        [,        ]]\n",
    "\n",
    "# formulate Accuracy by Confusion Matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix = metrics.confusion_matrix(y, y_pred1)\n",
    "# print(confusion_matrix)\n",
    "# plot_cm(y, y_pred, labels=['0', '1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How to devise metrics to solve the problem?\n",
    "<!-- We need Confusion Matrix to give more detailed information by  comparing predictions $y_\\text{pred}$ and ground truth $y$ -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What if the disease it detects is “CoVid” while there is a lack of doctors for diagnosis? \n",
    "  * So we are more likely to favour FP rather than FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# 1 represents the with-covid class; 0 represents the without-covid class\n",
    "y =      [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "# the first two predictions are FP\n",
    "y_pred1 = [1, 1, 0, 0, 1, 1, 1, 1] # 2 non covid -> covid\n",
    "# FP       *  * \n",
    "# print('Pred1:', metrics.accuracy_score(y, y_pred1))\n",
    "\n",
    "print(4/ 6 )\n",
    "\n",
    "\n",
    "# the first two predictions are FN\n",
    "y_pred2 = [0, 0, 0, 0, 1, 1, 1, 1]   # 3 covid ->  no covid\n",
    "# FN  \n",
    "print( 1/1)\n",
    "# print('Pred2:', metrics.accuracy_score(y, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The two metrics are called [Recall and Precision](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "    + We want to correctly detect actually Covid patients, i.e., aiming to decrease FN\n",
    "    + Recall $\\frac{TP}{TP+FN}$  (Sensitivity or TPR): hit rate.\n",
    "   \n",
    "    + but also want to reflect FP: false alarm (incorrectly detecting healthy patients as having Covid)\n",
    "    + Precision $\\frac{TP}{TP+FP}$\n",
    "    <!-- + Other metrics reflecting FP: ?? -->\n",
    "    <!-- Specificity=$\\frac{TN}{N}=\\frac{TN}{TN+FP}$ -->\n",
    "    + Opposite directions: the larger the metrics, the more FN/FP\n",
    "    <!-- missing rate: $\\frac{FN}{TP+FP}$ -->\n",
    "    <!-- FPR=$\\frac{FP}{N}=\\frac{FP}{TN+FP}$\n",
    "\n",
    "<!-- precision: how many examples are correctly predicted during all predictions for a target\n",
    "recall: how many examples are correctly predicted during all examples of one class -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Recall for FP predictions: ', metrics.recall_score(y, y_pred1))\n",
    "# print('Precision for FP predictions: ', metrics.precision_score(y, y_pred1))\n",
    "\n",
    "# print('Recall for FN predictions: ', metrics.recall_score(y, y_pred2))\n",
    "# print('Precision for FN predictions: ', metrics.precision_score(y, y_pred2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* F1: Balancing Recall and Precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(metrics.classification_report(y, y_pred, labels=[0, 1], target_names=[\"zero\",\"one\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How we can reduce misclassification for one class of examples? \n",
    "    + we always have a tradeoff between FN and FP. since, in reality, the model will output probabilities rather than directly give the class.\n",
    "*  How would reducing misclassification for one class of examples likely increase misclassification for another class of examples?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I take the demonstrated example from https://www.youtube.com/watch?v=4jRBRDbJemM&t=655s\n",
    "y =      np.array([0,   0,   0,   0,   1,   1,   1,   1])\n",
    "y_prob = np.array([0.5, 0.2, 0,   0,   0.5, 1,   1,   1])\n",
    "y_pred = y_prob >= 0.5\n",
    "print(y_pred)\n",
    "print(\"FN\", metrics.confusion_matrix(y, y_pred)[1,0])\n",
    "print(\"FP\", metrics.confusion_matrix(y, y_pred)[0,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ROC (Receiver Operating Characteristic) Curve reflecting the trade-off between FN and FP \n",
    "    + True Positive Rate (TPR, i.e., recall): 1 when FN is 0\n",
    "    + False Positive Rate (FPR): 0 when FP is 0; Which metric can be used to replace FPR?\n",
    "    + So the best point is (FPR, TPR) = (0, 1)\n",
    "    + (0, 0) is the extreme model for reducing FP\n",
    "    + (1, 1) is the extreme model for reducing FN\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(y, y_prob)\n",
    "print(thresholds)\n",
    "# Calculate AUC\n",
    "auc = metrics.auc(fpr, tpr)\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, marker='o', label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random guessing')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But they have different significaces in the real world.\n",
    "* In which cases we do not want people with disease midclassified as no disease, i.e., less FN?\n",
    "    + life threathening scenarios, e.g., heart disease, cancer\n",
    "    + disease easily causing outbreak, e.g., Covid\n",
    "    \n",
    "    \n",
    "* In which cases we do not want healthy people misclassified as with disease, i.e., less FP\n",
    "    + small problems causing unnecessary hospital cost, e.g., diarrhea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

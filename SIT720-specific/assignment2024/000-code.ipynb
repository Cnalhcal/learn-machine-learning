{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and explore the training and test dataset, do necessary pre-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.12.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/xinzheli/miniconda3/envs/learn_ml/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/xinzheli/miniconda3/envs/learn_ml/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/xinzheli/miniconda3/envs/learn_ml/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (1.4.1.post1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/xinzheli/miniconda3/envs/learn_ml/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/xinzheli/miniconda3/envs/learn_ml/lib/python3.12/site-packages (from imbalanced-learn->imblearn) (3.3.0)\n",
      "Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading imbalanced_learn-0.12.2-py3-none-any.whl (257 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.0/258.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.12.2 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "\n",
    "# Now X_resampled and y_resampled have balanced classes\n",
    "\n",
    "#load the cirrhosis_train.csv datafile\n",
    "\n",
    "train=pd.read_csv('data/D_TaskData/cirrhosis_train.csv')\n",
    "test=pd.read_csv('data/D_TaskData/cirrhosis_test.csv')\n",
    "        \n",
    "test_labels=pd.read_csv('data/D_TaskData/sampleprediction.csv')\n",
    "\n",
    "\n",
    "train.rename(columns={'trainID':'ID'}, inplace=True)\n",
    "test.rename(columns={'testID':'ID'}, inplace=True)\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "#drop the rows with missing values\n",
    "train=train.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Show both training and test dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195, 20)\n",
      "(88, 20)\n"
     ]
    }
   ],
   "source": [
    "#check the dataset size\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Based on the training and test data, show the feature types, and indicate which "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                 int64\n",
      "N_Days             int64\n",
      "Status            object\n",
      "Drug              object\n",
      "Age                int64\n",
      "Sex               object\n",
      "Ascites           object\n",
      "Hepatomegaly      object\n",
      "Spiders           object\n",
      "Edema             object\n",
      "Bilirubin        float64\n",
      "Cholesterol      float64\n",
      "Albumin          float64\n",
      "Copper           float64\n",
      "Alk_Phos         float64\n",
      "SGOT             float64\n",
      "Tryglicerides    float64\n",
      "Platelets        float64\n",
      "Prothrombin      float64\n",
      "Stage              int64\n",
      "dtype: object\n",
      "ID                 int64\n",
      "N_Days             int64\n",
      "Status           float64\n",
      "Drug              object\n",
      "Age                int64\n",
      "Sex               object\n",
      "Ascites           object\n",
      "Hepatomegaly      object\n",
      "Spiders           object\n",
      "Edema             object\n",
      "Bilirubin        float64\n",
      "Cholesterol      float64\n",
      "Albumin          float64\n",
      "Copper             int64\n",
      "Alk_Phos         float64\n",
      "SGOT             float64\n",
      "Tryglicerides    float64\n",
      "Platelets        float64\n",
      "Prothrombin      float64\n",
      "Stage              int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#print the feature types in the dataset\n",
    "\n",
    "print(train.dtypes)\n",
    "print(test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use an appropriate method to deal with the missing values for both the training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Do necessary encoding for the categorical features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID               int64\n",
      "N_Days           int64\n",
      "Status           int32\n",
      "Drug             int32\n",
      "Age              int64\n",
      "Sex              int32\n",
      "Ascites          int32\n",
      "Hepatomegaly     int32\n",
      "Spiders          int32\n",
      "Edema            int32\n",
      "Bilirubin        int64\n",
      "Cholesterol      int64\n",
      "Albumin          int64\n",
      "Copper           int64\n",
      "Alk_Phos         int64\n",
      "SGOT             int64\n",
      "Tryglicerides    int64\n",
      "Platelets        int64\n",
      "Prothrombin      int64\n",
      "Stage            int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "#encode categorical values\n",
    "train=train.apply(LabelEncoder().fit_transform)\n",
    "test=test.apply(LabelEncoder().fit_transform)\n",
    "\n",
    "\n",
    "print(train.dtypes)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Show the label distribution based on the training data, is it a balanced training \n",
    "set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'N_Days', 'Status', 'Drug', 'Age', 'Sex', 'Ascites',\n",
      "       'Hepatomegaly', 'Spiders', 'Edema', 'Bilirubin', 'Cholesterol',\n",
      "       'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets',\n",
      "       'Prothrombin', 'Stage'],\n",
      "      dtype='object')\n",
      "Status\n",
      "0    102\n",
      "2     81\n",
      "1     12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the column names in the 'train' dataframe\n",
    "print(train.columns)\n",
    "\n",
    "\n",
    "# Replace 'class' with the correct column name in the 'train' dataframe\n",
    "print(train['Status'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Based on the pre-processed training data from question 1, create three supervised \n",
    "machine learning (ML) models for predicting “Status”. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Use an appropriate validation method, report performance score using a suitable \n",
    "metric. Is it possible that the presented result is an underfitted or overfitted one? \n",
    "Justify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Model:\n",
      "Train Score: 1.0\n",
      "Test Score: 0.8064516129032258\n",
      "Generalization Error (Train-Test Gap): 0.19354838709677424\n",
      "Cross-validation Scores:\n",
      "Mean: 0.7221575885774723\n",
      "Standard Deviation: 0.034829339267962556\n",
      "\n",
      "Logistic Regression Model:\n",
      "Train Score: 0.7213114754098361\n",
      "Test Score: 0.7419354838709677\n",
      "Generalization Error (Train-Test Gap): -0.02062400846113166\n",
      "Cross-validation Scores:\n",
      "Mean: 0.650290851401375\n",
      "Standard Deviation: 0.05936271207560896\n",
      "\n",
      "Random Forest Model:\n",
      "Train Score: 1.0\n",
      "Test Score: 0.9354838709677419\n",
      "Generalization Error (Train-Test Gap): 0.06451612903225812\n",
      "Cross-validation Scores:\n",
      "Mean: 0.8889476467477525\n",
      "Standard Deviation: 0.04999523870882491\n"
     ]
    }
   ],
   "source": [
    "#create three supervised learning models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Set the parameter grid\n",
    "# param_grid_svc = {\n",
    "#     'C': [0.1, 1, 10],  # Regularization parameter\n",
    "#     'kernel': ['linear', 'rbf'],  # Type of hyperplane\n",
    "#     'gamma': ['scale', 'auto']  # Kernel coefficient\n",
    "# }\n",
    "#split the data into features and target\n",
    "X_train = train.drop(['ID', 'Status'], axis=1)\n",
    "y_train = train['Status']\n",
    "\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# # Create a GridSearchCV object\n",
    "# grid_search_svc = GridSearchCV(SVC(), param_grid_svc, cv=3, scoring='accuracy', verbose=1)\n",
    "# grid_search_svc.fit(X_train, y_train)\n",
    "\n",
    "# # Best parameters and best score\n",
    "# print(\"Best parameters:\", grid_search_svc.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search_svc.best_score_))\n",
    "\n",
    "\n",
    "# param_grid_lr = {\n",
    "#     'C': [0.01, 0.1, 1, 10],  # Inverse of regularization strength\n",
    "#     'solver': ['liblinear', 'saga']  # Algorithm to use in the optimization problem\n",
    "# }\n",
    "\n",
    "# grid_search_lr = GridSearchCV(LogisticRegression(max_iter=2000), param_grid_lr, cv=3, scoring='accuracy', verbose=1)\n",
    "# grid_search_lr.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search_lr.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search_lr.best_score_))\n",
    "\n",
    "\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "#     'max_features': ['auto', 'sqrt', 'log2'],  # Number of features to consider at every split\n",
    "#     'max_depth': [None, 10, 20, 30]  # Maximum number of levels in tree\n",
    "# }\n",
    "\n",
    "# grid_search_rf = GridSearchCV(RandomForestClassifier(class_weight='balanced'), param_grid_rf, cv=3, scoring='accuracy', verbose=1)\n",
    "# grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# print(\"Best parameters:\", grid_search_rf.best_params_)\n",
    "# print(\"Best cross-validation score: {:.2f}\".format(grid_search_rf.best_score_))\n",
    "\n",
    "\n",
    "#train the support vector machine model\n",
    "# Train and evaluate the SVC model\n",
    "SVC_model = SVC(C=10, gamma='auto', kernel='rbf')\n",
    "SVC_model.fit(X_train, y_train)\n",
    "SVC_train_score = SVC_model.score(X_train, y_train)\n",
    "SVC_test_score = SVC_model.score(X_test, y_test)\n",
    "SVC_cv_scores = cross_val_score(SVC_model, X_resampled, y_resampled, cv=5)\n",
    "print(\"SVC Model:\")\n",
    "print(\"Train Score:\", SVC_train_score)\n",
    "print(\"Test Score:\", SVC_test_score)\n",
    "print(\"Generalization Error (Train-Test Gap):\", SVC_train_score - SVC_test_score)\n",
    "print(\"Cross-validation Scores:\")\n",
    "print(\"Mean:\", SVC_cv_scores.mean())\n",
    "print(\"Standard Deviation:\", SVC_cv_scores.std())\n",
    "\n",
    "# Train and evaluate the Logistic Regression model\n",
    "LR_model = LogisticRegression(max_iter=2000, C=10, solver='liblinear')\n",
    "LR_model.fit(X_train, y_train)\n",
    "LR_train_score = LR_model.score(X_train, y_train)\n",
    "LR_test_score = LR_model.score(X_test, y_test)\n",
    "LR_cv_scores = cross_val_score(LR_model, X_resampled, y_resampled, cv=5)\n",
    "print(\"\\nLogistic Regression Model:\")\n",
    "print(\"Train Score:\", LR_train_score)\n",
    "print(\"Test Score:\", LR_test_score)\n",
    "print(\"Generalization Error (Train-Test Gap):\", LR_train_score - LR_test_score)\n",
    "print(\"Cross-validation Scores:\")\n",
    "print(\"Mean:\", LR_cv_scores.mean())\n",
    "print(\"Standard Deviation:\", LR_cv_scores.std())\n",
    "\n",
    "# Train and evaluate the Random Forest model\n",
    "RF_model = RandomForestClassifier(class_weight='balanced', max_depth=10, max_features='log2', n_estimators=200)\n",
    "RF_model.fit(X_train, y_train)\n",
    "RF_train_score = RF_model.score(X_train, y_train)\n",
    "RF_test_score = RF_model.score(X_test, y_test)\n",
    "RF_cv_scores = cross_val_score(RF_model, X_resampled, y_resampled, cv=5)\n",
    "print(\"\\nRandom Forest Model:\")\n",
    "print(\"Train Score:\", RF_train_score)\n",
    "print(\"Test Score:\", RF_test_score)\n",
    "print(\"Generalization Error (Train-Test Gap):\", RF_train_score - RF_test_score)\n",
    "print(\"Cross-validation Scores:\")\n",
    "print(\"Mean:\", RF_cv_scores.mean())\n",
    "print(\"Standard Deviation:\", RF_cv_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC Model:\n",
    "\n",
    "- The model achieves a perfect train score of 1.0, indicating that it fits the training data perfectly.\n",
    "- However, the test score is lower at 0.8065, suggesting that the model may be overfitting to the training data.\n",
    "- The generalization error (train-test gap) of 0.1935 is relatively high, further confirming the overfitting issue.\n",
    "- The mean cross-validation score of 0.7222 is lower than the test score, indicating that the model's performance is not consistent across different folds of the data.\n",
    "- The standard deviation of cross-validation scores is relatively low at 0.0348, suggesting that the model's performance is somewhat stable across different folds.\n",
    "\n",
    "\n",
    "Logistic Regression Model:\n",
    "\n",
    "- The train score of 0.7213 and test score of 0.7419 are relatively close, indicating that the model is not overfitting or underfitting significantly.\n",
    "- The generalization error (train-test gap) is negative (-0.0206), which means the model performs slightly better on the test data than on the training data. This could be due to random variations or the specific split of the data.\n",
    "- The mean cross-validation score of 0.6503 is lower than both the train and test scores, suggesting that the model's performance may not be as good when evaluated on different folds of the data.\n",
    "- The standard deviation of cross-validation scores is relatively high at 0.0594, indicating that the model's performance varies considerably across different folds.\n",
    "\n",
    "\n",
    "Random Forest Model:\n",
    "\n",
    "- The model achieves a perfect train score of 1.0, indicating that it fits the training data perfectly.\n",
    "- The test score of 0.9355 is high, suggesting that the model generalizes well to unseen data.\n",
    "- The generalization error (train-test gap) of 0.0645 is relatively low, indicating that the model is not severely overfitting.\n",
    "- The mean cross-validation score of 0.8922 is close to the test score, suggesting that the model's performance is consistent across different folds of the data.\n",
    "- The standard deviation of cross-validation scores is moderate at 0.0445, indicating some variability in the model's performance across different folds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Justify different design decisions for each ML model used to answer this \n",
    "question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC (Support Vector Classifier):** This model is chosen for its effectiveness in high-dimensional spaces and its capability to model complex nonlinear relationships. The decision to use SVC might be rooted in its ability to find the optimal hyperplane that best separates the data into classes, which is critical in medical or health-related predictions where accuracy is paramount.\n",
    "**Logistic Regression:** This model is a straightforward, probabilistic approach suitable for binary classification tasks. It’s used here likely due to its simplicity and the interpretability of its results, which is important in clinical settings where understanding the influence of variables is as important as the prediction itself.\n",
    "**Random Forest:** This ensemble method is utilized for its robustness and ability to handle overfitting, which is common in complex models. By aggregating the results of multiple decision trees, it generally provides high accuracy and handles imbalanced data well, particularly with the use of 'balanced' for class weights to adjust for potential biases in dataset size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Have you optimised any hyper-parameters for each ML model? What are they? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVC (Support Vector Classifier):**\n",
    "\n",
    "\n",
    "- **C:** Set to 10 to manage the trade-off between achieving a low-error classifier and a decision boundary that is as simple as possible. Higher C values give more flexibility to the model, allowing it to better capture the complexity of the data.\n",
    "- **gamma:** Kept as 'auto', which automatically adjusts based on the number of features to avoid over-fitting.\n",
    "- **kernel:** Selected 'rbf' (radial basis function) for its effectiveness in dealing with non-linear boundaries, making the model more adaptive to complex patterns.\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "\n",
    "- **max_iter:** Maintained at 2000 to ensure the algorithm has enough iterations to converge, especially useful when dealing with complex data.\n",
    "- **C:** Increased to 10, reducing the regularization strength, which allows the model to increase its complexity and potentially improve fit.\n",
    "- **solver:** Chosen 'liblinear' which is a good choice for small datasets and binary classification, and it handles L1 penalty, useful for feature selection.\n",
    "\n",
    "**Random Forest:**\n",
    "\n",
    "- **n_estimators:** Set to 200 to increase the number of trees in the forest, providing a better ensemble model that is less prone to overfitting.\n",
    "- **max_features:**'log2' is selected, reducing the number of features considered for splitting at each leaf node, which helps in reducing overfitting and increasing the diversity among the trees.\n",
    "- **max_depth:** Capped at 10 to control the depth of each tree, ensuring the trees do not grow deep enough to fit the noise in the data, which helps in generalizing better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What can you do with the label imbalance issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my implementation I oversampled the lower class and for the random forest I added weighting for the classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Finally, make a model recommendation based on the reported results and justify \n",
    "it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd recommend the random forest classifier based on the fact that it has  higher metric scores than the other two models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the best model that you get from question 2, do prediction on the pre-processed test \n",
    "set. Save your prediction (the prediction should contain two columns only: testID and \n",
    "Status), and submit it to the specific Kaggle in-class platform, do a screenshot of your \n",
    "model performance and report it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to predicted_status.csv\n"
     ]
    }
   ],
   "source": [
    "# Prepare  test data\n",
    "X_test = test.drop(['ID', 'Status'], axis=1)\n",
    "\n",
    "# Predict using the trained model\n",
    "test_predictions = RF_model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame with IDs and Predictions\n",
    "output_df = pd.DataFrame({\n",
    "    'testID': test['ID']+1,\n",
    "    'Predicted_Status': test_predictions\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = \"predicted_status.csv\"\n",
    "output_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"CSV file saved to {output_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

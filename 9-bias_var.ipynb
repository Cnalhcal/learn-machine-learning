{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier as skDecisionTreeClassifier\n",
    "from my_ml_package.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from my_ml_package.data.synthetic import variance_sample_for_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "* Question: BIG or SMALL?\n",
    "  |                          | Training Loss/Error/(1-Accuracy)                 | Test Loss/Error/(1-Accuracy)                     |\n",
    "  |------------------------------------|-----------------------------------|-----------------------------------|\n",
    "  |Overfitting    |     |   |\n",
    "  |Underfitting   |     |   |\n",
    "* Generalization\n",
    "  * Generalization Error or Test Error\n",
    "    <!-- \\operatorname{Err}_{\\mathcal{D}_\\text{tr}}= -->\n",
    "    $$\\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test})) \\mid Y_\\text{train}, X_\\text{train}]$$ \n",
    "    \n",
    "  <!-- * Expected Generalization Error\n",
    "    $$\\operatorname{Err}=\\mathrm{E}[L(Y_\\text{test}, \\hat{f}(X_\\text{test}))]=\\mathrm{E}\\left[\\operatorname{Err}_{\\text{train}}\\right]$$  -->\n",
    "  * Generalization Gap >0\n",
    "    $$ \\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test}))] - \\mathrm{E}[\\operatorname{Err}(Y_\\text{train}, f(X_\\text{train}))]$$\n",
    "    $$  \\mathrm{E}[\\operatorname{Acc}(Y_\\text{train}, f(X_\\text{train}))] - \\mathrm{E}[ \\operatorname{Acc}(Y_\\text{test}, f(X_\\text{test}))] $$\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = variance_sample_for_decision_tree(sample_size = 40, num_features=5, corr=0.95,) # we need some variabiliy in the data for bootstrap to work\n",
    "X_test, y_test = variance_sample_for_decision_tree(sample_size = 20, num_features=5, corr=0.95,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.75\n"
     ]
    }
   ],
   "source": [
    "treeclf = DecisionTreeClassifier(max_depth=15)\n",
    "treeclf.fit(X_train, y_train) # feature_idx2name={i: str(i) for i in range(X_train.shape[1])}\n",
    "y_train_pred = treeclf.predict(X_train)\n",
    "y_pred = treeclf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(accuracy_score(y_train, treeclf.predict(X_train))))\n",
    "print('The accuracy on test data is {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "# print('The test error rate is {:.2f}'.format(1 - accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias & Variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  1.0\n",
      "Accuracy train:  0.975\n",
      "Generalization gap:  -0.025000000000000022\n"
     ]
    }
   ],
   "source": [
    "# week 6\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logistic_model_iris = LogisticRegression(max_iter=200)\n",
    "logistic_model_iris.fit(X_train, y_train)\n",
    "y_pred_test = logistic_model_iris.predict(X_test)\n",
    "y_pred_train = logistic_model_iris.predict(X_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy test: \", accuracy_test)\n",
    "print(\"Accuracy train: \", accuracy_train)\n",
    "print(\"Generalization gap: \", accuracy_train - accuracy_test)\n",
    "\n",
    "# week 8\n",
    "# df = pd.read_csv('data/Data_for_UCI_named.csv')\n",
    "# X = df.drop('stabf', axis=1).to_numpy()\n",
    "# y = df['stabf'].to_numpy()\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "# dt_model = DecisionTreeClassifier()\n",
    "# dt_model.fit(X_train, y_train)\n",
    "# y_pred = dt_model.predict(X_test)\n",
    "# print(\"Accuracy test: \", accuracy_test)\n",
    "# print(\"Accuracy train: \", accuracy_train)\n",
    "# print(\"Generalization gap: \", accuracy_train - accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the 100% test accuracy due to the easy examples from splitting or because the ML algorithm fits the task??**\n",
    "\n",
    "\n",
    "* Looking for multiple test sets for expectation?\n",
    "* Looking for the expected ability of the ML model (Do not fix training data but do fix the task)  \n",
    "    $$\\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test})) \\mid Y_\\text{train}, X_\\text{train}]$$ \n",
    "    $$\\Rightarrow \\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test}))$$\n",
    "    $$= \\mathrm{E}\\left[(y-\\hat{y})^2\\right]$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 1.        , 0.93333333, 0.96666667, 1.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = cross_val_score(logistic_model_iris, X, y, cv=5, scoring='accuracy')\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance**\n",
    "* Question: What does the term \"variance\" in the bias-variance tradeoff refer to?\n",
    "  1.  refer to the variation across different test samples given a fixed model\n",
    "  2.  refer to the variability of the learning algorithm across different training subsets. \n",
    "   \n",
    "**Bias**\n",
    "* Question: What does the term \"bias\" in the bias-variance tradeoff refer to?\n",
    "  1.  a model's inability to capture the true relationship\n",
    "  2.  a real-world problem (which may be complex) is approximated by a simplified model.\n",
    "\n",
    "\n",
    "**Bias and Variance Tell you Where the Error Come from Inherently?**\n",
    "  $$\n",
    "  \\mathrm{E}\\left[(y-\\hat{y})^2\\right]= (f^{*}(x)+\\epsilon-\\hat{y})^2 = (\\mathrm{E}[\\hat{y}]-f^{*}(x))^2+\\mathrm{E}\\left[(\\hat{y}-\\mathrm{E}[\\hat{y}])^2\\right]+\\sigma^2\n",
    "  $$\n",
    " \n",
    "  * $ \\operatorname{Bias}(\\hat{y})=\\mathrm{E}[\\hat{y}]-f^{*}(x)$\n",
    "  * $ \\operatorname{Var}(\\hat{y})=\\mathrm{E}\\left[(\\hat{y}-\\mathrm{E}[\\hat{y}])^2\\right]$\n",
    "\n",
    "<!-- * $\\sigma^2$: the variance of the noise \n",
    "  * To make it useful, personally, I will assume $f^{*}(x)=y$ -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span c=\"red\">HIDE SOME CONTENT </span>\n",
    "\n",
    "<!-- * High Variance\n",
    "  * How to measure **Variance** of the learning algorithm?\n",
    "    * Test Error / Accuracy\n",
    "    * Generalization Gap between Training Data and Test Data\n",
    "    * Deviations across the different validation sets via 5-fold CV\n",
    "    * Variability of the test performance across different bootstrapped training samples\n",
    "  * Overfitting: a model learns the training data too well, including its noise and specific details that don't generalize to unseen data.\n",
    "  \n",
    "* Overfitting v.s. Variance\n",
    "  * **Overfitting and variance are not the same.** Overfitting is a condition, while variance is a property of the model.\n",
    "  * Overfitting is often a symptom of high variance, meaning a model that is too complex becomes overly sensitive to the training data specifics and doesn't generalize well to new data. \n",
    "\n",
    "  | Setting | Model (1)                          | Training Data (2)                  | Test Data (3)                      |\n",
    "    |---------|------------------------------------|-----------------------------------|-----------------------------------|\n",
    "    | 1       | <span style=\"color:red;\">Fix</span>        | <span style=\"color:red;\">Fix</span>         | <span style=\"color:green;\">Vary</span>      |\n",
    "    | 2       | <span style=\"color:red;\">Fixed</span>   | <span style=\"color:green;\">Vary</span>      | <span style=\"color:green;\">Vary</span>      |\n",
    "\n",
    "* High Bias\n",
    "  *  Concrete Example: A linear model trying to fit a nonlinear dataset would typically have high bias because it cannot capture the nonlinear relationships.\n",
    "*  Underfitting v.s. Bias\n",
    "   *  Underfitting happens when a model is too simple to learn the patterns in the training data properly, resulting in poor performance both on training and test data. -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40\n",
      "Variance: 0.04216888888888889\n"
     ]
    }
   ],
   "source": [
    "def report_variance(model, X_train, y_train, X_test, y_test):\n",
    "    # Prepare 5-Fold Cross Validation\n",
    "    kf = KFold(n_splits=30, shuffle=True, random_state=42)\n",
    "    predictions = []\n",
    "\n",
    "    # Train models and collect predictions\n",
    "    print(\"Train size:\", len(X_train))\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Collect the predicted probabilities for each test instance\n",
    "        pred = model.predict(X_test)\n",
    "        predictions.append(pred)\n",
    "\n",
    "\n",
    "    # Convert to a numpy array for easier processing\n",
    "    predictions = np.array(predictions)\n",
    "    # Calculate Variance\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    print(f\"Variance: {variance}\")\n",
    "    # Overfit -> High Vairance\n",
    "    # Decdision  Tree -> 0.109\n",
    "    # Random Forest -> 0.0421\n",
    "\n",
    "    # # Calculate Bias \n",
    "    # mean_predictions = np.mean(predictions, axis=0) # Expected prediction E[y_hat]\n",
    "    # assert mean_predictions.shape == (len(X_test),)\n",
    "    # bias = np.mean((mean_predictions - y_test) ** 2)\n",
    "    # print(f\"Bias: {bias}\")\n",
    "\n",
    "# model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "# model = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", max_features=None, max_depth=2).fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sit720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

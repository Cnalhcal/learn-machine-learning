{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from my_ml_package.data.synthetic import variance_sample_for_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting\n",
    "* Question: BIG or SMALL?\n",
    "  |                          | Training Loss/Accuracy/Error                 | Test Loss/Accuracy/Error                     |\n",
    "  |------------------------------------|-----------------------------------|-----------------------------------|\n",
    "  |Overfitting    |  Small   | Big  |\n",
    "  |Underfitting   |  Big   |  Big |\n",
    "* Generalization\n",
    "  * Generalization Error or Test Error\n",
    "    <!-- \\operatorname{Err}_{\\mathcal{D}_\\text{tr}}= -->\n",
    "    $$\\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test})) \\mid Y_\\text{train}, X_\\text{train}]$$ \n",
    "    \n",
    "  <!-- * Expected Generalization Error\n",
    "    $$\\operatorname{Err}=\\mathrm{E}[L(Y_\\text{test}, \\hat{f}(X_\\text{test}))]=\\mathrm{E}\\left[\\operatorname{Err}_{\\text{train}}\\right]$$  -->\n",
    "  * Generalization Gap >0\n",
    "    $$ \\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test}))] - \\operatorname{Err}(Y_\\text{train}, f(X_\\text{train}))$$\n",
    "    $$ \\operatorname{Acc}(Y_\\text{train}, f(X_\\text{train})) - \\mathrm{E}[ \\operatorname{Acc}(Y_\\text{test}, f(X_\\text{test}))] $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  1.0\n",
      "Accuracy train:  0.975\n",
      "Generalization gap:  -0.025000000000000022\n"
     ]
    }
   ],
   "source": [
    "# week 6\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logistic_model_iris = LogisticRegression(max_iter=200)\n",
    "logistic_model_iris.fit(X_train, y_train)\n",
    "y_pred_test = logistic_model_iris.predict(X_test)\n",
    "y_pred_train = logistic_model_iris.predict(X_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "print(\"Accuracy test: \", accuracy_test)\n",
    "print(\"Accuracy train: \", accuracy_train)\n",
    "print(\"Generalization gap: \", accuracy_train - accuracy_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy test:  1.0\n",
      "Accuracy train:  0.975\n",
      "Generalization gap:  -0.025000000000000022\n"
     ]
    }
   ],
   "source": [
    "# week 8\n",
    "df = pd.read_csv('data/Data_for_UCI_named.csv')\n",
    "X = df.drop('stabf', axis=1).to_numpy()\n",
    "y = df['stabf'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "y_pred = dt_model.predict(X_test)\n",
    "print(\"Accuracy test: \", accuracy_test)\n",
    "print(\"Accuracy train: \", accuracy_train)\n",
    "print(\"Generalization gap: \", accuracy_train - accuracy_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the 100% test accuracy due to the easy examples from splitting?**\n",
    "* Looking for multiple test sets for expectation?\n",
    "* Looking for the expected ability of the ML model (Do not fix training data but do fix the task)  \n",
    "    $$\\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test})) \\mid Y_\\text{train}, X_\\text{train}]$$ \n",
    "    $$\\Rightarrow \\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test}))$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9995, 1.    , 0.9995, 1.    , 1.    ])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(dt_model, X, y, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where does the error come from?**\n",
    "$$\\mathrm{E}[\\operatorname{Err}(Y_\\text{test}, f(X_\\text{test}))=\\mathrm{E}\\left[(y-\\hat{y})^2\\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on training data is 1.00\n",
      "The accuracy on test data is 0.61\n",
      "The test error rate is 0.39\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = variance_sample_for_decision_tree(sample_size = 40) # we need some variabiliy in the data for bootstrap to work\n",
    "X_test, y_test = variance_sample_for_decision_tree(sample_size = 2000)\n",
    "\n",
    "treeclf = DecisionTreeClassifier()\n",
    "treeclf.fit(X_train, y_train)\n",
    "y_train_pred = treeclf.predict(X_train)\n",
    "y_pred = treeclf.predict(X_test)\n",
    "print('The accuracy on training data is {:.2f}'.format(accuracy_score(y_train, treeclf.predict(X_train))))\n",
    "print('The accuracy on test data is {:.2f}'.format(accuracy_score(y_test, y_pred)))\n",
    "print('The test error rate is {:.2f}'.format(1 - accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias & Variance\n",
    "* Question: What does the term \"variance\" in the bias-variance tradeoff refer to?\n",
    "  1.  refer to the variation across different test samples given a fixed model\n",
    "  2.  refer to the variability of the learning algorithm across different training subsets. \n",
    "\n",
    "\n",
    "* Question: What does the term \"bias\" in the bias-variance tradeoff refer to?\n",
    "  1.  a model's inability to capture the true relationship\n",
    "  2.  a real-world problem (which may be complex) is approximated by a simplified model.\n",
    "\n",
    "* Formal Definition under Regression:\n",
    "  $$\n",
    "  \\mathrm{E}\\left[(y-\\hat{y})^2\\right]= (f^{*}(x)+\\epsilon-\\hat{y})^2 = (\\mathrm{E}[\\hat{y}]-f^{*}(x))^2+\\mathrm{E}\\left[(\\hat{y}-\\mathrm{E}[\\hat{y}])^2\\right]+\\sigma^2\n",
    "  $$\n",
    " \n",
    "  * $ \\operatorname{Bias}(\\hat{y})=\\mathrm{E}[\\hat{y}]-f^{*}(x)$\n",
    "  * $ \\operatorname{Var}(\\hat{y})=\\mathrm{E}\\left[(\\hat{y}-\\mathrm{E}[\\hat{y}])^2\\right]$\n",
    "  <!-- * $\\sigma^2$: the variance of the noise -->\n",
    "  * To make it useful, personally, I will assume $f^{*}(x)=y$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* High Variance\n",
    "  * How to measure **Variance** of the learning algorithm?\n",
    "    * Test Error / Accuracy\n",
    "    * Generalization Gap between Training Data and Test Data\n",
    "      <!-- * can suggest overfitting (and thus high variance), it isn't a direct measure of variance itself -->\n",
    "    * Deviations across the different validation sets via 5-fold CV\n",
    "    * Variability of the test performance across different bootstrapped training samples\n",
    "  * Overfitting: a model learns the training data too well, including its noise and specific details that don't generalize to unseen data.\n",
    "  \n",
    "* Overfitting v.s. Variance\n",
    "  * **Overfitting and variance are not the same.** Overfitting is a condition, while variance is a property of the model.\n",
    "  * Overfitting is often a symptom of high variance, meaning a model that is too complex becomes overly sensitive to the training data specifics and doesn't generalize well to new data. \n",
    "  <!-- * Reducing overfitting typically involves reducing variance by techniques like regularization, cross-validation, pruning, or using ensemble methods. -->\n",
    "  | Setting | Model (1)                          | Training Data (2)                  | Test Data (3)                      |\n",
    "    |---------|------------------------------------|-----------------------------------|-----------------------------------|\n",
    "    | 1       | <span style=\"color:red;\">Fix</span>        | <span style=\"color:red;\">Fix</span>         | <span style=\"color:green;\">Vary</span>      |\n",
    "    | 2       | <span style=\"color:red;\">Fixed</span>   | <span style=\"color:green;\">Vary</span>      | <span style=\"color:green;\">Vary</span>      |\n",
    "\n",
    "<!-- * The prediction error (light red curves) $\\operatorname{Err}_{\\mathcal{T}}$ for 100 simulated training sets each of size 50. The lasso was used to produce the sequence of fits.\n",
    "    <center><img src=\"pics/bias_variance.png\" width=\"500\"></center> -->\n",
    "<!-- * Actually, the gap between training sample and test sample is commonly larger than the performance gaps between test samples. -->\n",
    "\n",
    "\n",
    "\n",
    "* High Bias\n",
    "  *  Concrete Example: A linear model trying to fit a nonlinear dataset would typically have high bias because it cannot capture the nonlinear relationships.\n",
    "*  Underfitting v.s. Bias\n",
    "   *  Underfitting happens when a model is too simple to learn the patterns in the training data properly, resulting in poor performance both on training and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 40\n",
      "Variance: 0.04216888888888889\n"
     ]
    }
   ],
   "source": [
    "# Prepare 5-Fold Cross Validation\n",
    "kf = KFold(n_splits=30, shuffle=True, random_state=42)\n",
    "predictions = []\n",
    "\n",
    "# Train models and collect predictions\n",
    "print(\"Train size:\", len(X_train))\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "    \n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "    model = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", max_features=None, max_depth=2).fit(X_train, y_train)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Collect the predicted probabilities for each test instance\n",
    "    pred = model.predict(X_test)\n",
    "    predictions.append(pred)\n",
    "\n",
    "\n",
    "# Convert to a numpy array for easier processing\n",
    "predictions = np.array(predictions)\n",
    "# Calculate Variance\n",
    "variance = np.mean(np.var(predictions, axis=0))\n",
    "print(f\"Variance: {variance}\")\n",
    "# Overfit -> High Vairance\n",
    "# Decdision  Tree -> 0.109\n",
    "# Random Forest -> 0.0421\n",
    "\n",
    "# # Calculate Bias \n",
    "# mean_predictions = np.mean(predictions, axis=0) # Expected prediction E[y_hat]\n",
    "# assert mean_predictions.shape == (len(X_test),)\n",
    "# bias = np.mean((mean_predictions - y_test) ** 2)\n",
    "# print(f\"Bias: {bias}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sit720",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"markdown","metadata":{},"source":["# jacobian for a vector-valued function\n","```\n","line 1: model = nn.Linear(100, 2)\n","# forward pass of model, i.e., chain of functions\n","line 2: batch = torch.randn(16, 100, requires_grad=False)\n","line 3: expected_output = torch.randn(16, 10, requires_grad=False)\n","line 4: output = model(batch)\n","line 5: loss = (expected_output - output).pow(2).sum()\n","line 6: loss.backward()\n","line 7: optimizer = torch.optim.SGD(model.parameters(), lr=0.001) # give weights to `optimizer` to 'learn'\n","line 8: optimizer.step()\n","```\n","\n","In this notebook, I will discuss\n","* **how the back propagation is performed in Pytorch AutoGrad to compute the gradient? i.e., line 4-6 in the above code**\n","\n","I will not discuss\n","* how the gradients are computed in theory\n","* optimization details, i.e., line 7-8 in the above code \n","    + Assume we all know the general idea that parameters of neural network are updated via gradient descent algorithm after computing gradients. But there are many details or variations of gradient descent.\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-10-19T05:06:01.625237Z","iopub.status.busy":"2021-10-19T05:06:01.624886Z","iopub.status.idle":"2021-10-19T05:06:01.630384Z","shell.execute_reply":"2021-10-19T05:06:01.62917Z","shell.execute_reply.started":"2021-10-19T05:06:01.625206Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import math\n","import torch.nn.functional as F"]},{"cell_type":"markdown","metadata":{},"source":["## Tensorflow AutoGrad"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = tf.Variable(4.0)\n","\n","def func(x):\n","  return 2*x\n","\n","with tf.GradientTape() as t:\n","  y = func(x)\n","\n","dy_dx = t.gradient(y, x)\n","\n","dy_dx.numpy()"]},{"cell_type":"markdown","metadata":{},"source":["## `Tensor` for AutoGrad\n","Relevant attributes for computing gradients\n","* `requires_grad`: to contral whether the gradient during the backward\n","* `grad_fn`: after performing operations (e.g., sin) on the input tensor, the `grad_fn` attribute would be added into the output tensor in order to calculate gradients afterward.\n","    + input tensor has no `grad_fn`: Since it is defined by user\n","    \n","See the following code for details."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:02.356395Z","iopub.status.busy":"2021-10-19T05:06:02.355797Z","iopub.status.idle":"2021-10-19T05:06:02.363839Z","shell.execute_reply":"2021-10-19T05:06:02.362381Z","shell.execute_reply.started":"2021-10-19T05:06:02.356347Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<SinBackward0 object at 0x106f16b20>\n"]}],"source":["# create an input tensor\n","input1 = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n","\n","# grad_fn for output tensor\n","out1= torch.sin(input1)\n","print(out1.grad_fn)\n","\n","# input tensor has no grad_fn\n","# print(input.grad_fn)"]},{"cell_type":"markdown","metadata":{},"source":["## AutoGrad\n","[The official introduction](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#computational-graph) is shown below. \n","\n","\n","Recording Tensor Operations: Computational Graph\n","* keeping a record of data (tensors) & all executed\n","operations (along with the resulting new tensors) in a directed acyclic\n","graph (DAG) \n","  + leaves are the input tensors \n","  + roots are the output tensors \n","  + By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n","* See supported PyTorch [`Function`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n"]},{"cell_type":"markdown","metadata":{},"source":["\n","$$Q=3 a^3-b^2$$\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from torch.utils.tensorboard import SummaryWriter\n","\n","# Define tensors\n","a = torch.tensor([2., 3.], requires_grad=True)\n","b = torch.tensor([6., 4.], requires_grad=True)\n","\n","# Define the function\n","Q = 3 * a**3 - b**2\n","\n","# Setup the SummaryWriter\n","writer = SummaryWriter('runs/graph_visualization')\n","\n","# Add the graph to TensorBoard\n","writer.add_graph(Q, input_to_model=None)\n","writer.close()\n","\n","# Compute the gradients\n","external_grad = torch.tensor([1., 1.])\n","Q.backward(gradient=external_grad)\n"]},{"cell_type":"markdown","metadata":{},"source":["\n",">\n","> In a forward pass, autograd does two things simultaneously:\n","> \n","> - run the requested operation to compute a resulting tensor, and\n","> - maintain the operation’s *gradient function* in the DAG.\n",">\n","> The backward pass kicks off when ``.backward()`` is called on the DAG\n","root. ``autograd`` then:\n","> \n","> - computes the gradients from each ``.grad_fn``,\n","> - accumulates them in the respective tensor’s ``.grad`` attribute, and\n","> - using the chain rule, propagates all the way to the leaf tensors.\n","> \n","> Below is a visual representation of the DAG in our example. In the graph,\n","the arrows are in the direction of the forward pass. The nodes represent the backward functions\n","of each operation in the forward pass. The leaf nodes in blue represent our leaf tensors ``a`` and ``b``.\n","> \n","> .. figure:: /_static/img/dag_autograd.png\n","> \n","> <div class=\"alert alert-info\"><h4>Note</h4><p>**DAGs are dynamic in PyTorch**\n","  An important thing to note is that the graph is recreated from scratch; after each\n","  ``.backward()`` call, autograd starts populating a new graph. This is\n","  exactly what allows you to use control flow statements in your model;\n","  you can change the shape, size and operations at every iteration if\n","  needed.</p></div>\n","> \n","> Exclusion from the  Directed Acyclic Graph (DAG)\n",">\n","> ``torch.autograd`` tracks operations on all tensors which have their\n","``requires_grad`` flag set to ``True``. For tensors that don’t require\n","gradients, setting this attribute to ``False`` excludes it from the\n","gradient computation DAG.\n",">\n","> The output tensor of an operation will require gradients even if only a\n","single input tensor has ``requires_grad=True``.\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Optimizer: Controling the Use of Grad"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Gradient before step: tensor([-4.])\n","Updated parameter: tensor([1.], requires_grad=True)\n"]}],"source":["from torch import optim\n","\n","# Initialize a parameter tensor\n","param = torch.tensor([1.0], requires_grad=True)\n","\n","# Create an optimizer and pass the parameter to it\n","# optimizer = optim.SGD([param], lr=1)\n","loss = (param - 3)**2\n","loss.backward()\n","print(\"Gradient before step:\", param.grad)\n","\n","# optimizer.zero_grad()\n","# optimizer.step()\n","\n","# Check the updated parameter value\n","print(\"Updated parameter:\", param)\n"]},{"cell_type":"markdown","metadata":{},"source":["### `.backward` only works for scaler loss"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:03.121634Z","iopub.status.busy":"2021-10-19T05:06:03.120618Z","iopub.status.idle":"2021-10-19T05:06:03.128373Z","shell.execute_reply":"2021-10-19T05:06:03.127192Z","shell.execute_reply.started":"2021-10-19T05:06:03.121575Z"},"trusted":true},"outputs":[],"source":["# code from the previous cell\n","param1 = torch.linspace(0., 2. * math.pi, steps=25, requires_grad=True)\n","out1= torch.sin(param1)\n","\n","# perform more operations\n","out2 = 2 * out1\n","out3 = out2 + 1\n","\n","#  call `backward` to calculate the gradient so the ERROR occurs\n","# since auto differentiation requires the final output to be a single scalar value\n","# The following section will explain why we cannot directly calculate jacobian for a vector valued function (i.e., tensor output)\n","# and how we calculate it\n","# out3.backward() \n","\n","# Therefore, we aggregate out3 into a scalar and call backward implicitly\n","out4 = out3.sum()  \n","out4.backward()\n","\n","# Now, no problem\n","# after backward, intermediate result will be freed"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:03.532247Z","iopub.status.busy":"2021-10-19T05:06:03.531621Z","iopub.status.idle":"2021-10-19T05:06:03.538508Z","shell.execute_reply":"2021-10-19T05:06:03.537773Z","shell.execute_reply.started":"2021-10-19T05:06:03.532206Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([ 2.0000e+00,  1.9319e+00,  1.7321e+00,  1.4142e+00,  1.0000e+00,\n","         5.1764e-01, -8.7423e-08, -5.1764e-01, -1.0000e+00, -1.4142e+00,\n","        -1.7321e+00, -1.9319e+00, -2.0000e+00, -1.9319e+00, -1.7321e+00,\n","        -1.4142e+00, -1.0000e+00, -5.1764e-01,  2.3850e-08,  5.1764e-01,\n","         1.0000e+00,  1.4142e+00,  1.7321e+00,  1.9319e+00,  2.0000e+00])"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Now, check the gradient we get for the input tensor (or variable)\n","param1.grad"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:03.922764Z","iopub.status.busy":"2021-10-19T05:06:03.922108Z","iopub.status.idle":"2021-10-19T05:06:03.928246Z","shell.execute_reply":"2021-10-19T05:06:03.927255Z","shell.execute_reply.started":"2021-10-19T05:06:03.922724Z"},"trusted":true},"outputs":[],"source":["# Now, further look how `grad_fn` works to achieve this gradient\n","# `grad_fn` has an attribute  `next_functions`\n","# which stores all the backward functions for each forward operands\n","# For example, out3 is calculated by Add(out2 ,1)\n","# Therefore, `next_functions` has two elements: MulBackward0 for `out2` and None for`1` \n","out3.grad_fn.next_functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:04.398815Z","iopub.status.busy":"2021-10-19T05:06:04.398105Z","iopub.status.idle":"2021-10-19T05:06:04.409351Z","shell.execute_reply":"2021-10-19T05:06:04.40817Z","shell.execute_reply.started":"2021-10-19T05:06:04.398764Z"},"trusted":true},"outputs":[],"source":["# Note that MulBackward0 is exactly `out2.grad_fn`\n","assert out3.grad_fn.next_functions[0][0] == out2.grad_fn\n","\n","grad_fn_3 = out3.grad_fn\n","assert grad_fn_3 == out3.grad_fn\n","print(grad_fn_3) # out3 <-- Add(out2 ,1)\n","\n","grad_fn_2 = grad_fn_3.next_functions[0][0] # <- 2 * out1 \n","assert grad_fn_2 == out2.grad_fn\n","print('grad_fn_2:', grad_fn_2) \n","\n","grad_fn_1 = grad_fn_2.next_functions[0][0] # <- sin(input1)\n","assert grad_fn_1 == out1.grad_fn\n","print('grad_fn_1:', grad_fn_1)\n","\n","grad_fn_0 = grad_fn_1.next_functions[0][0]  # <- accumulate\n","assert input1.grad_fn is None\n","print('grad_fn_0:',grad_fn_0)"]},{"cell_type":"markdown","metadata":{},"source":["## Vector-Jacobian Product\n","In the previous cell, we cannot calculate jacobian. In order to do this, We need to explicitly pass a gradient argument in `out3.backward()` which is a tensor of the same shape as out3. For example, we can pass `1` to represent the gradient of out3 w.r.t. itself: `out3.backward(gradient=torch.ones(out3.shape))`. The gradient of the input tensor will equal to sum up `out3` into a scalar. To emphasize the main point,\n","\n","**`AutoGrad` works using Vector-Jacobian Product**\n","\n","This is exactly what happens during training a neural network where **any intermediate gradient in the model** is jacobian, **Loss gradient** is the vector.\n","\n","Specifically, a forward pass during training consists of:\n","* function1: y=f(x) (e.g.,multi-class classifiers C):\n","    + Forward: n-dim input x -> m-dim output y\n","    + Backward: get `Jacobian(y w.r.t x)`  -> **Model output gradient**\n","* function2: loss fn L(y): \n","    + Forward: m-dim output -> scalar value\n","    + Backward: get the `GradientVector(L w.r.t y)`, -> **Loss gradient**\n","* Matrix Multiplication: Jacobian(y w.r.t x) * GradientVector(L w.r.t y)\n","    + Chain rule: get grad(L w.r.t x) -> **Loss gradient**\n","        \n","        \n","\n","[The official introduction](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) is shown below.\n","\n","> ## Vector Calculus using ``autograd``\n","> Mathematically, if you have a vector valued function\n","$\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with\n","respect to $\\vec{x}$ is a Jacobian matrix $J$:\n",">\n","> \\begin{align}J\n","     =\n","      \\left(\\begin{array}{cc}\n","      \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n","      ... &\n","      \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n","      \\end{array}\\right)\n","     =\n","     \\left(\\begin{array}{ccc}\n","      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n","      \\vdots & \\ddots & \\vdots\\\\\n","      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","      \\end{array}\\right)\\end{align}\n",">\n","> Generally speaking, ``torch.autograd`` is an engine for computing\n","vector-Jacobian product. That is, given any vector $\\vec{v}$, compute the product\n","$J^{T}\\cdot \\vec{v}$\n",">\n","> If $\\vec{v}$ happens to be the gradient of a scalar function $l=g\\left(\\vec{y}\\right)$:\n",">\n","> \\begin{align}\\vec{v}\n","   =\n","   \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\\end{align}\n",">\n","> then by the chain rule, the vector-Jacobian product would be the\n","gradient of $l$ with respect to $\\vec{x}$:\n",">\n","> \\begin{align}J^{T}\\cdot \\vec{v}=\\left(\\begin{array}{ccc}\n","      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n","      \\vdots & \\ddots & \\vdots\\\\\n","      \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n","      \\end{array}\\right)\\left(\\begin{array}{c}\n","      \\frac{\\partial l}{\\partial y_{1}}\\\\\n","      \\vdots\\\\\n","      \\frac{\\partial l}{\\partial y_{m}}\n","      \\end{array}\\right)=\\left(\\begin{array}{c}\n","      \\frac{\\partial l}{\\partial x_{1}}\\\\\n","      \\vdots\\\\\n","      \\frac{\\partial l}{\\partial x_{n}}\n","      \\end{array}\\right)\\end{align}\n",">\n","> This characteristic of vector-Jacobian product is what we use in the above example;\n","``external_grad`` represents $\\vec{v}$."]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:13.474997Z","iopub.status.busy":"2021-10-19T05:06:13.474639Z","iopub.status.idle":"2021-10-19T05:06:13.483358Z","shell.execute_reply":"2021-10-19T05:06:13.482383Z","shell.execute_reply.started":"2021-10-19T05:06:13.474962Z"},"trusted":true},"outputs":[],"source":["import torch\n","x = torch.Tensor([1.,2.,3.])\n","x.requires_grad = True\n","y = x * 2\n","print(y)\n","\n","# we have to give the vector for jacobian calculation\n","# i.e., set manual gradient of the tensor which is backward from\n","v = torch.tensor([1.,0.1,0.01], dtype=torch.float)\n","y.backward(gradient=v)\n","print(x.grad)\n","print(x.grad_fn)\n","print(x.is_leaf)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:15.05086Z","iopub.status.busy":"2021-10-19T05:06:15.050235Z","iopub.status.idle":"2021-10-19T05:06:15.064247Z","shell.execute_reply":"2021-10-19T05:06:15.063159Z","shell.execute_reply.started":"2021-10-19T05:06:15.050821Z"},"trusted":true},"outputs":[],"source":["# How a batch of samples （e.g., [x1, x2]） accumulate gradients? \n","import torch\n","# parameters\n","w = torch.Tensor([1.,2.,3.])\n","print('Original w grad:', w.grad)\n","w.requires_grad=True\n","# we create `x` containing two examples, each has 3-dimensional features\n","x1 = torch.Tensor([1.,2.,3.])\n","x2 = torch.Tensor([4.,5.,6.])\n","\n","y = x1 * w\n","loss = y.sum()\n","loss.backward()\n","print('w grad with x1:', w.grad)\n","\n","y = x2 * w\n","loss = y.sum()\n","w.grad = None\n","loss.backward()\n","print('w grad with x2:', w.grad)\n","\n","\n","# The gradients will be summed up in the batch dimension\n","x = torch.Tensor([[1.,2.,3.],\n","                  [4.,5.,6.]])\n","y = x * w\n","loss = y.sum()\n","w.grad = None\n","loss.backward()\n","print('mini-batch gradient of parameters with x1 and x2:', w.grad)"]},{"cell_type":"markdown","metadata":{},"source":["## `nn.Module` for AutoGrad\n","Neural network cosists of a stack of operations on data input tensors and model parameter tensors (This is explained by [my previous post](https://www.kaggle.com/sergioli212/neural-network-pytorch-module)). `nn.Module` has the basic implementation to record the model parameters and operations in high level. Then, these parameter tensors use `AutoGrad` to manage the gradient track so that parameters could be updated once they get their gradients from `backward`. \n","\n","In a nutshell, \n","\n","**All the neural networks in Pytorch are built upon the parent class `nn.Module`**, further, **in a nested way**\n","\n","The following code cell demonstrates how model parameters are used by `Module` and how they are **nested**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:17.222015Z","iopub.status.busy":"2021-10-19T05:06:17.221339Z","iopub.status.idle":"2021-10-19T05:06:17.239554Z","shell.execute_reply":"2021-10-19T05:06:17.23842Z","shell.execute_reply.started":"2021-10-19T05:06:17.221965Z"},"trusted":true},"outputs":[],"source":["class TinyModel(nn.Module):\n","    def __init__(self):\n","        super(TinyModel, self).__init__()\n","        \n","        self.layer1 = nn.Linear(1000, 100)\n","        self.relu = nn.ReLU()\n","        self.layer2 = nn.Linear(100, 10)\n","        \n","    def forward(self, x):\n","        return self.layer2(self.relu(self.layer1(x)))\n","\n","model = TinyModel()\n","\n","\n","# `weight` -> `torch.nn.parameter.Parameter`\n","# `Parameter` is the sub-class of `Tensor`\n","print(type(model.layer2.weight))\n","# These `Tensor`s are leaf nodes. So they do not have `grad_fn`\n","print(model.layer2.weight.grad_fn)\n","# but slice is considered as differentiable function\n","print(model.layer2.weight[0][:10]) \n","# no grad before `backward`\n","print(model.layer2.weight.grad)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:18.48526Z","iopub.status.busy":"2021-10-19T05:06:18.484759Z","iopub.status.idle":"2021-10-19T05:06:18.519011Z","shell.execute_reply":"2021-10-19T05:06:18.518173Z","shell.execute_reply.started":"2021-10-19T05:06:18.485225Z"},"trusted":true},"outputs":[],"source":["# forward pass of model, i.e., chain of functions\n","batch = torch.randn(16, 1000, requires_grad=False)\n","expected_output = torch.randn(16, 10, requires_grad=False)\n","output = model(batch)\n","loss = (expected_output - output).pow(2).sum()\n","\n","# backward pass\n","loss.backward()"]},{"cell_type":"markdown","metadata":{},"source":["Turn `Autograd` off"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:22.286922Z","iopub.status.busy":"2021-10-19T05:06:22.28639Z","iopub.status.idle":"2021-10-19T05:06:22.301536Z","shell.execute_reply":"2021-10-19T05:06:22.300698Z","shell.execute_reply.started":"2021-10-19T05:06:22.286883Z"},"trusted":true},"outputs":[],"source":["\n","# with Autograd\n","a = torch.ones(2, 3, requires_grad=True)\n","b = torch.ones(2, 3, requires_grad=True)\n","c = a * b\n","print(c)\n","\n","# turn Autograd off only when both `a` and `b` are off\n","a.requires_grad = False\n","c = a * b\n","# print(c.grad_fn.next_functions) # it only has one `grad_fn` for `b` now \n","print(c)\n","\n","\n","a.requires_grad = False\n","b.requires_grad = False\n","c = a * b\n","print(c) # no `grad_fn`\n","\n","a.requires_grad = True\n","b.requires_grad = True\n","# turn Autograd off temporarily using `no_grad()` context manager\n","# note that in contrast to `no_grad()`, we can use `enable_grad()` to turn on\n","with torch.no_grad():\n","    c = a + b\n","print(c)\n","\n","c = a + b\n","print(c)\n","\n","# turn Autograd off in function using `no_grad` decorator\n","@torch.no_grad()\n","def add_tensors(x, y):\n","    return x + y\n","\n","\n","# copy tensor w.o computation history using `detach()`\n","# this happens mostly for needs to implicit conversion\n","# between tensor and numpy array in libraries like matplotlib\n","x = torch.randn(5, requires_grad=True)\n","y = x.detach()\n","print(x)\n","print(y)"]},{"cell_type":"markdown","metadata":{},"source":["## Advanced Topics\n","### Backward Hook in PyTorch\n","\n","While the gradients of model parameters could be accessed directly via `grad` attribute, sometimes we also need to access the gradient of model/layer input or output which provide information for a lot of use cases like visualization, or, in my case, crafting adversarial examples.\n","\n","Some Resources:  \n","\n","https://stackoverflow.com/questions/65011884/understanding-backward-hooks\n","\n","[Gradients from hook triggered by backward and autograd are different](https://discuss.pytorch.org/t/gradients-from-hook-triggered-by-backward-and-autograd-are-different/68209)\n","\n","https://www.kaggle.com/sironghuang/understanding-pytorch-hooks\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:24.361196Z","iopub.status.busy":"2021-10-19T05:06:24.360677Z","iopub.status.idle":"2021-10-19T05:06:24.368137Z","shell.execute_reply":"2021-10-19T05:06:24.367347Z","shell.execute_reply.started":"2021-10-19T05:06:24.361159Z"},"trusted":true},"outputs":[],"source":["class TestModule(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super().__init__()\n","        internal_size = 5\n","        self.linear1 = nn.Linear(input_size, internal_size)\n","        self.linear2 = nn.Linear(internal_size, output_size)\n","\n","    def forward(self, x):\n","        x = self.linear1(x)\n","        x = F.relu(x)\n","        x = self.linear2(x)\n","        return x\n","model = TestModule(10,2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:30.766506Z","iopub.status.busy":"2021-10-19T05:06:30.766023Z","iopub.status.idle":"2021-10-19T05:06:30.771257Z","shell.execute_reply":"2021-10-19T05:06:30.770074Z","shell.execute_reply.started":"2021-10-19T05:06:30.766472Z"},"trusted":true},"outputs":[],"source":["# model = TestModule(4, 3)\n","\n","\n","# def print_hook(module: nn.Module, _inputs, _outputs):\n","#     print('hook triggered on', module)\n","\n","\n","# for module in model.modules():\n","#     if isinstance(module, nn.Linear):\n","#         module.register_full_backward_hook(print_hook)\n","#         print('added hook to', module)\n","\n","# x, y = torch.rand(size=(20, 4)), torch.randint(low=0, high=3, size=(20,))\n","# loss = F.cross_entropy(model(x), y)\n","\n","# print('backward started')\n","# loss.backward()\n","# print('backward complete')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-10-19T05:06:40.849773Z","iopub.status.busy":"2021-10-19T05:06:40.849118Z","iopub.status.idle":"2021-10-19T05:06:40.854507Z","shell.execute_reply":"2021-10-19T05:06:40.853428Z","shell.execute_reply.started":"2021-10-19T05:06:40.849722Z"},"trusted":true},"outputs":[],"source":["# # embedding layer\n","# embedding_layer = nn.Embedding(1000, 768)\n","# embedding_layer(torch.randint(0, embedding_layer.weight.shape[0], (8, 33))) # no grad_in, only grad_out for input embeddings\n","\n","\n","# # linear layer\n","# L = nn.Linear(5, 10)\n","# A = nn.Sequential(L, nn.Linear(10, 20), nn.Linear(20, 1))\n","\n","# out_grads = []\n","# in_grads = []\n","# def grad_hook(module, grad_in, grad_out):\n","#     in_grads.append(grad_in[0])\n","#     out_grads.append(grad_out[0])\n","\n","# emb_hook = L.register_full_backward_hook(grad_hook)\n","# from torch.autograd import Variable\n","# X = Variable(torch.randn(3, 5), requires_grad=True)\n","# loss = A(X).sum()\n","# loss.backward()\n","# # print(in_grads[0].shape)\n","# # print(out_grads[0].shape)\n","\n","# # grad_in: (3, 5), grad_out: (3, 10)\n","# #  grad_in: (50000, 768); grad_out: (8, 33, 768)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Batch Normalization\n",
    "How it works: Batch normalization normalizes the data across the batch dimension. It computes the mean and variance for each feature over the entire batch. For each feature, the mean and variance are used to normalize the data. This is usually done before the activation function in each layer.\n",
    "\n",
    "Usage: Commonly used in convolutional neural networks (CNNs) and fully connected layers. It is particularly effective in vision tasks.\n",
    "\n",
    "#### Pros:\n",
    "\n",
    "Improves gradient flow: It helps in managing the vanishing and exploding gradient problems.\n",
    "Allows higher learning rates: By stabilizing the learning process, it often allows for the use of higher learning rates, which can speed up training.\n",
    "Reduces the strong dependence on initialization.\n",
    "\n",
    "#### Cons:\n",
    "\n",
    "Dependency on batch size: Its effectiveness can degrade with smaller batch sizes, as the estimates of the mean and variance become less reliable.\n",
    "Performance variability: In tasks with high variability in batch data (like different image types in a single batch), batch normalization may perform inconsistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "How it works: Imagine a layer of a neural network producing a set of activations for a single input. The mean and variance are computed across all the neurons (or activation outputs) within a single layer for each individual data point. Here's how the process breaks down:\n",
    "\n",
    "1. **Calculation of Statistics**: For a given input in the network, after it has passed through a layer (but before it is activated, in some implementations), the mean and variance are computed across all the activations produced by that layer for this specific input. \n",
    "\n",
    "2. **Normalization**: Each activation \\( x_i \\) is then normalized using the formula:\n",
    "   $$\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "   $$\n",
    "   where \\( \\mu \\) is the mean and \\( \\sigma^2 \\) is the variance of the activations for that input, and \\( \\epsilon \\) is a small constant added for numerical stability (to avoid division by zero).\n",
    "\n",
    "3. **Scaling and Shifting**: Finally, the normalized activations \\( \\hat{x}_i \\) are scaled and shifted using learned parameters \\( \\gamma \\) and \\( \\beta \\) (specific to each layer). This allows the layer to undo the normalization if that turns out to be beneficial. The final output \\( y_i \\) of the normalization step is then:\n",
    "   $$\n",
    "   y_i = \\gamma \\hat{x}_i + \\beta\n",
    "   $$\n",
    "\n",
    "This entire process ensures that for each input, the outputs of a layer are standardized across the features or neurons within that layer, leading to more stable distribution of activations. This helps mitigate the internal covariate shift problem and makes training more predictable and stable across different layers and depths in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from my_ml_package.stat import var, covar, covar_matrix\n",
    "from my_ml_package.visualize import plot_data_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the underlying idea of PCA for dimensionality reduction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# People data with Height cm, Weight kg\n",
    "people = np.array([\n",
    "    [175, 75],  # Adult    1\n",
    "    [60, 5],    # Baby     \n",
    "    [50, 4],    # Baby     1\n",
    "    [70, 7],    # Baby     \n",
    "    [180, 80],  # Adult    1\n",
    "    [178, 72],  # Adult (new) 1\n",
    "    [172, 70],  # Adult (new)  \n",
    "    [169, 74],  # Adult (new)  \n",
    "    [55, 6],    # Baby (new)  \n",
    "    [65, 8]     # Baby (new)  1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the frequency of hangouts with Friend A and Friend B within a week.\n",
    "hangout = np.array([\n",
    "   #Friend A     Friend B\n",
    "        [2,       4],    # Me\n",
    "        [10,      2],    # Person 1\n",
    "        [20,      3]     # Person 2\n",
    "])\n",
    "\n",
    "#     -M1      -M2\n",
    "#      P    P\n",
    "#      P    N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 5 2]\n",
      "[0 0 2 5]\n"
     ]
    }
   ],
   "source": [
    "# Document(rows)-word(columns) matrix\n",
    "data = np.array([\n",
    "    # good superb\n",
    "    [0,    0],  # D1: Neutral or negative\n",
    "    [3,    0],  # D2: Moderately positive\n",
    "    [5,    2],  # D3: Positive\n",
    "    [2,    5]   # D4: Highly positive\n",
    "])\n",
    "\n",
    "good = data[:,0]\n",
    "superb = data[:,1]\n",
    "print(good)\n",
    "print(superb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-document matrix\n",
    "# Document 1 (D1): Negative sentiment\n",
    "# Document 2 (D2): Positive sentiment\n",
    "# Document 3 (D3): Strongly Positive sentiment\n",
    "# A = np.array([\n",
    "#     # D1   D2   D3\n",
    "#      [3,   0,   0], # bad\n",
    "#      [0,   2,   1], # good\n",
    "#      [0,   1,   3]  # superb\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(5, 3))\n",
    "# ax.hist(good, bins=15, alpha=1, label=f'Var(good)={np.var(good):.2f}')\n",
    "# ax.hist(superb, bins=15, alpha=0.5, label=f'Var(superb)= {np.var(superb):.2f}')\n",
    "# ax.set_title('Variance Demonstration')\n",
    "# ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datapoints on each dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Data points for the two versions\n",
    "# good = [0, 3, 5, 2]\n",
    "# superb = [0, 0, 2, 5]\n",
    "# excellent = [0, 1, 2, 3]\n",
    "# print('Good: Means=', np.mean(good), 'Var=', np.var(good))\n",
    "# print('Superb: Means=', np.mean(superb), 'Var=', np.var(superb))\n",
    "# print('Excellent: Means=', np.mean(excellent), 'Var=', np.var(excellent))\n",
    "\n",
    "# # Adjusting the plot to prevent overlapping by offsetting the two versions on separate axes\n",
    "# plt.figure(figsize=(10, 2))  # Adjusting figure size for clarity\n",
    "\n",
    "# # Offsetting the two versions slightly on the y-axis to prevent overlapping\n",
    "# y_offset = 0.1  # Offset amount\n",
    "\n",
    "# # Plot for the good version as points, offset upwards\n",
    "# plt.plot(good, [-2*y_offset]*len(good), 'x', label='Good (0, 3, 5, 2)', markersize=10, color='green')\n",
    "# plt.axvline(np.mean(good), color='green', linestyle='--',)\n",
    "\n",
    "# # Plot for the original version as points, offset downwards\n",
    "# plt.plot(superb, [-y_offset]*len(superb), 'o', label='Superb (0, 0, 2, 3)', markersize=10, color='blue')\n",
    "# plt.axvline(np.mean(superb), color='blue', linestyle='--', )\n",
    "\n",
    "\n",
    "# # Plot for the add-1 version as points, offset upwards\n",
    "# plt.plot(excellent, [y_offset]*len(excellent), 's', label='Excellent (0, 1, 2, 3)', markersize=10, color='red')\n",
    "# plt.axvline(np.mean(excellent), color='red', linestyle='--', )\n",
    "\n",
    "# # Removing y-axis and ticks as the focus is on the values' spread\n",
    "# plt.yticks([])\n",
    "# plt.xticks(range(4))  # Assuming the possible range of values for illustration\n",
    "# plt.xlabel('Values')\n",
    "\n",
    "# # Adding a legend to distinguish the versions\n",
    "# plt.legend()\n",
    "\n",
    "# # Show grid on the x-axis to indicate value positions\n",
    "# plt.grid(True, axis='x')\n",
    "\n",
    "# # Display the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance Matrix\n",
    "* What is variance?\n",
    "* What is covariance?\n",
    "* What is covariance matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.333333333333333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def covar(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "    n = len(x)\n",
    "    return sum((x - x_mean) * (y - y_mean)) / (n - 1)\n",
    "\n",
    "covar(data[:, 0], data[:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.500000010063152"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good =  data[:,0]\n",
    "superb = data[:, 1]\n",
    "\n",
    "def covar(x, y):\n",
    "    x_mean = np.mean(x)\n",
    "    y_mean = np.mean(y)\n",
    "\n",
    "    return np.sum((x-x_mean) * (y-y_mean)) / (len(x)-1)\n",
    "\n",
    "def covar_matrix(data):\n",
    "    n = data.shape[1]\n",
    "    covar_matrix = np.zeros((n, n))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            covar_matrix[i, j] = covar(data[:, i], data[:, j])\n",
    "\n",
    "    return covar_matrix\n",
    "    \n",
    "covar_matrix(data)\n",
    "\n",
    "#            # good                      superb\n",
    "#         [  \n",
    "# # good       Var(good)                  covar(good, superb)\n",
    "# # superb     covar( superb, good)         Var(superb)\n",
    "#          ]\n",
    "# Our goal: a vector to do the dot product with the original features\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue, Eigenvecor\n",
    "<!-- We can do Eigen-decomposition on a symmetric matrix $A$\n",
    "<!-- * $AV=VD$\n",
    "* Importantly, if A is a symmetric matrix, $v_1, ..., v_d$ are orthogonal to each other. **V is an orthogonal matrix**\n",
    "* D is a diagonal matrix\n",
    "* $A=VDV^T$  -->\n",
    "\n",
    "\n",
    "* all the eigenvalues of $A$ are nonnegative $A$ is positive semi-difinite\n",
    "\n",
    "$$\n",
    "A=P D P^{-1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $A$ is the square matrix being decomposed.\n",
    "- $P$ is a matrix whose columns are the eigenvectors of $A$.\n",
    "- $D$ is a diagonal matrix whose diagonal elements are the eigenvalues of $A$.\n",
    "- $P^{-1}$ is the inverse of matrix $P$. -->\n",
    "\n",
    "$ A p = \\lambda p$,  where $p \\in \\mathbb{R}^{d}$\n",
    "$$\n",
    "\\left[\\begin{array}{cccc}\n",
    "A_{11} & A_{12} & \\cdots & A_{1 d} \\\\\n",
    "A_{21} & A_{22} & \\cdots & A_{2 d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{d 1} & A_{d 2} & \\cdots & A_{d d}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "p_1 \\\\\n",
    "p_2 \\\\\n",
    "\\vdots \\\\\n",
    "p_d\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "* $A$: A general square matrix. In your description, it's used interchangeably with $S$, but typically $A$ can represent any square matrix, not necessarily symmetric.\n",
    "* $v$: An eigenvector of $A$. In the context given, it plays the same role as $p$ but for the matrix $A$.\n",
    "* $v_i$: The $i^{th}$ element of the eigvector $v$ (or $p$).\n",
    "* $A_{ij}$: The element of matrix $A$ (or $S$) in the $i^{th}$ row and $j^{th}$ column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalue, Eigenvecor\n",
    "<!-- We can do Eigen-decomposition on a symmetric matrix $A$\n",
    "<!-- * $AV=VD$\n",
    "* Importantly, if A is a symmetric matrix, $v_1, ..., v_d$ are orthogonal to each other. **V is an orthogonal matrix**\n",
    "* D is a diagonal matrix\n",
    "* $A=VDV^T$  -->\n",
    "\n",
    "\n",
    "* all the eigenvalues of $A$ are nonnegative $A$ is positive semi-difinite\n",
    "\n",
    "$$\n",
    "A=P D P^{-1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $A$ is the square matrix being decomposed.\n",
    "- $P$ is a matrix whose columns are the eigenvectors of $A$.\n",
    "- $D$ is a diagonal matrix whose diagonal elements are the eigenvalues of $A$.\n",
    "- $P^{-1}$ is the inverse of matrix $P$. -->\n",
    "\n",
    "$ A p = \\lambda p$,  where $p \\in \\mathbb{R}^{d}$\n",
    "$$\n",
    "\\left[\\begin{array}{cccc}\n",
    "A_{11} & A_{12} & \\cdots & A_{1 d} \\\\\n",
    "A_{21} & A_{22} & \\cdots & A_{2 d} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "A_{d 1} & A_{d 2} & \\cdots & A_{d d}\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "p_1 \\\\\n",
    "p_2 \\\\\n",
    "\\vdots \\\\\n",
    "p_d\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "* $A$: A general square matrix. In your description, it's used interchangeably with $S$, but typically $A$ can represent any square matrix, not necessarily symmetric.\n",
    "* $v$: An eigenvector of $A$. In the context given, it plays the same role as $p$ but for the matrix $A$.\n",
    "* $v_i$: The $i^{th}$ element of the eigvector $v$ (or $p$).\n",
    "* $A_{ij}$: The element of matrix $A$ (or $S$) in the $i^{th}$ row and $j^{th}$ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix: \n",
      "[[4.33333333 0.83333333]\n",
      " [0.83333333 5.58333333]]\n",
      "Eigen values: [6.         3.91666667]\n",
      "Eigen vectors: [[-0.4472136  -0.89442719]\n",
      " [-0.89442719  0.4472136 ]]\n",
      "Reduced Feature:  [ 0.         -1.3416408  -4.02492238 -5.36656315]\n"
     ]
    }
   ],
   "source": [
    "def covar(x, y):\n",
    "    \"\"\" Covariance of two variables.\n",
    "    It measures how two variables change together by \n",
    "     the average of the product of the differences of each data point from the sample mean.\n",
    "    Equation: cov(x, y) = Σ (x_i - mean(x)) * (y_i - mean(y)) / n\n",
    "    Args:\n",
    "        x, y (np.ndarray): Two NumPy arrays of the same length.\"\"\"\n",
    "    return np.sum((x - x.mean())*(y - y.mean()))/len(x)\n",
    "\n",
    "def covar_matrix(X):\n",
    "    \"\"\" Covariance matrix of a dataset.\n",
    "    It is a square matrix that describes the covariance between two or more variables in a dataset.\n",
    "    Args:\n",
    "        X (np.ndarray): A NumPy array of shape (n_samples, n_features).\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    covar_matrix = np.zeros((n_features, n_features))\n",
    "    for i in range(n_features):\n",
    "        for j in range(n_features):\n",
    "            covar_matrix[i, j] = covar(X[:, i], X[:, j])\n",
    "    return covar_matrix\n",
    "\n",
    "# plot eigenvectors on the data plot marked by components\n",
    "def plot_eigenvectors(data, eigenvector):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(data[:,0], data[:,1])\n",
    "    # add label on the data points\n",
    "    labels = ['Negative', 'Positive', 'Moderately Positive', 'Strongly Positive']\n",
    "    for i in range(data.shape[0]):\n",
    "        ax.text(data[i,0], data[i,1], labels[i], fontsize=6, ha='right')\n",
    "    for i in range(2):\n",
    "        ax.quiver(0, 0, eigenvector[i,0], eigenvector[i,1], angles='xy', scale_units='xy', scale=1, color='red')\n",
    "        ax.text(eigenvector[i,0], eigenvector[i,1], f'Component {i+1}', fontsize=6)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim(-1, 6)\n",
    "    ax.set_ylim(-1, 6)\n",
    "    plt.show()\n",
    "\n",
    "# Step1: Computing covariance matrix\n",
    "# data = (data - data.mean(axis=0)) \n",
    "covmat=covar_matrix(data) * len(data) / (len(data) - 1)\n",
    "\n",
    "# Step2: Performing EigenDecomposition\n",
    "lambd, eigenvector= np.linalg.eig(covmat)\n",
    "\n",
    "# Step3: Ordering the eigenvalues and eigenvectors\n",
    "idx = lambd.argsort()[::-1]\n",
    "lambd = lambd[idx]\n",
    "eigenvector = eigenvector[:,idx]\n",
    "print('Covariance matrix: \\n{}'.format(covmat))\n",
    "print('Eigen values: {}'.format(lambd))\n",
    "print('Eigen vectors: {}'.format(eigenvector))\n",
    "# plot_eigenvectors(data, eigenvector)\n",
    "# Covariance matrix: \n",
    "# [[4.33333333 0.83333333]\n",
    "#  [0.83333333 5.58333333]]\n",
    "# Eigen values: [6.         3.91666667]\n",
    "# Eigen vectors: [[-0.4472136  -0.89442719]\n",
    "#  [-0.89442719  0.4472136 ]]\n",
    "principal_components = np.array([[-0.4472136,  -0.89442719],  # principal component 1\n",
    " [-0.89442719,  0.4472136 ]]) # principal component 2\n",
    "# 6. \n",
    "print(\"Reduced Feature: \", data.dot(principal_components))\n",
    "\n",
    "# print(np.var() * len(data) / (len(data) - 1)) # 6.000000013417536\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- without centering, PCA might capture variance related to the offset from the origin.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "# Step4: Projecting the data onto the new feature space\n",
    "# matrix propagation\n",
    "data_pca = np.dot(data-data.mean(axis=0), eigenvector) \n",
    "# print(data_pca)\n",
    "print(covar(data_pca[:,0], data_pca[:,1]))\n",
    "# 2.220446049250313e-16-> approximating to 0\n",
    "\n",
    "# print('Variance of PCA transformed data: {}, {}'.format(var(data_pca1), var(data_pca2)))\n",
    "# print('Explained Variance Ratio: {}'.format(lambd/np.sum(lambd)))\n",
    "# print('Variance of PCA transformed data: {}, {}'.format(var(data_pca1)*len(data)/(len(data)-1), var(data_pca2)*len(data)/(len(data)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.25 -1.25  0.75  1.75]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Data points for the two versions\n",
    "original_version = [0, 0, 2, 3] - np.mean([0, 0, 2, 3])\n",
    "print(original_version)\n",
    "projected_version = list(data_pca1)\n",
    "\n",
    "# Adjusting the plot to prevent overlapping by offsetting the two versions on separate axes\n",
    "plt.figure(figsize=(10, 2))  # Adjusting figure size for clarity\n",
    "\n",
    "# Offsetting the two versions slightly on the y-axis to prevent overlapping\n",
    "y_offset = 0.1  # Offset amount\n",
    "\n",
    "# Plot for the original version as points, offset downwards\n",
    "plt.plot(original_version, [-y_offset]*len(original_version), 'o', label='Feature with Maximum Variance', markersize=10, color='blue')\n",
    "\n",
    "# Plot for the add-1 version as points, offset upwards\n",
    "plt.plot(projected_version, [y_offset]*len(projected_version), 's', label='Porjection on the 1st component ', markersize=10, color='red')\n",
    "\n",
    "# Removing y-axis and ticks as the focus is on the values' spread\n",
    "plt.yticks([])\n",
    "plt.xticks(range(4))  # Assuming the possible range of values for illustration\n",
    "plt.xlabel('Values')\n",
    "\n",
    "# Adding a legend to distinguish the versions\n",
    "plt.legend()\n",
    "\n",
    "# Show grid on the x-axis to indicate value positions\n",
    "plt.grid(True, axis='x')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# verify the result: covmat*eigenvector = lambd*eigenvector\n",
    "# print('Verify the result: covmat*eigenvector = lambd*eigenvector')\n",
    "# print(np.dot(covmat, eigenvector))\n",
    "# print(lambd*eigenvector)\n",
    "\n",
    "# let's understand  covmat . S P=P D\n",
    "# print(np.dot(covmat, eigenvector[:, 0])) # (d, d) * (d, 1)\n",
    "# print(eigenvector[:, 0]*lambd[0]) # (d, 1)\n",
    "\n",
    "# check whether eigenvector is orthogonal\n",
    "# np.dot(eigenvector[:, 0], eigenvector[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In assignment, you can simply use Scikit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    # good superb\n",
    "    [0,    0],  # D1: Neutral or negative\n",
    "    [3,    0],  # D2: Moderately positive\n",
    "    [5,    2],  # D3: Positive\n",
    "    [2,    5]   # D4: Highly positive\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigen values: [6.         3.91666667]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# covariance, eigenvalues and eigenvectors from sklearn PCA\n",
    "pca = PCA(n_components=2, random_state=2)\n",
    "pca.fit(data)\n",
    "covmat = pca.get_covariance()\n",
    "lambd = pca.explained_variance_ \n",
    "# print('Covariance matrix: \\n{}'.format(covmat))\n",
    "# print('Explained Variance Ratio: {}'.format(pca.explained_variance_ratio_))\n",
    "print('Eigen values: {}'.format(lambd))\n",
    "# print('Eigen vectors: {}'.format(eigenvector))\n",
    "# transform the data\n",
    "data_pca = pca.transform(data)\n",
    "# print('Variance of PCA transformed data: {}, {}'.format(var(data_pca[:,0]), var(data_pca[:,1])))\n",
    "# Covariance matrix: \n",
    "# [[4.33333333 0.83333333]\n",
    "#  [0.83333333 5.58333333]]\n",
    "# Eigen values: [6.         3.91666667]\n",
    "# Eigen vectors: [[-0.4472136  -0.89442719]\n",
    "#  [-0.89442719  0.4472136 ]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar(data_pca[:,0], data_pca[:,1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove correlation using Eigenvector\n",
    "* Project data into new coordinates\n",
    "    * eigenvalues/eigenvectors are not actually ordered typically but in this case they are and the first eigenvalue accounts for the most variance. By that standard it is superior.\n",
    "* Why does the transformation of eigenvector remove the correlation?\n",
    "    + \"covariance among each pair of the principal axes is zero (the principal axes are uncorrelated i.e. they are orthogonal to each other).\" (copy from 4.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The orginal covariance/unnormalized correlations:\n",
      "[[1.        0.7355261]\n",
      " [0.7355261 1.       ]]\n",
      "The covariance/unnormalized correlations after removing correlation:\n",
      "[[1.73552610e+00 2.87461239e-16]\n",
      " [2.87461239e-16 2.64473903e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Z=X^T P\n",
    "Z = np.dot(Xnorm, eigenvector) # (n, D) * (D, d) D = d\n",
    "eigenvector.shape\n",
    "# Z # (D, D)\n",
    "\n",
    "# check covariance\n",
    "X = Xnorm\n",
    "print(\"The orginal covariance/unnormalized correlations:\")\n",
    "print(np.dot(X.T, X)/len(X) -  np.dot(X.mean(axis=0, keepdims=True).T , X.mean(axis=0, keepdims=True)))\n",
    "\n",
    "print(\"The covariance/unnormalized correlations after removing correlation:\")\n",
    "X = Z\n",
    "print(np.dot(X.T, X)/len(X) -  np.dot(X.mean(axis=0, keepdims=True).T , X.mean(axis=0, keepdims=True)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to the idea of dimensionality reduction\n",
    "* Could we use distance metrics we learnt from last week?\n",
    "        $$\n",
    "        \\mathcal{L}=\\sum_{i<j}\\left(d_{i j}-\\left\\|\\mathbf{y}_i-\\mathbf{y}_j\\right\\|\\right)^2\n",
    "        $$\n",
    "        + How many computations we need?\n",
    "        + Projection-based method\n",
    "* t-SNE: $p_{i j}=\\frac{w_{i j}}{Z}$ is the normalized similarity between points i and j. \n",
    "    + Two closed neighbours in the high-dimensional space -> large $p_{i j}$. Clearly, it gives high price for putting closd neighbours far away. \n",
    "    + How about the price to put points far away closed? \n",
    "    + How to define $q_{i j}$? \n",
    "    + How close neighbours attract each other while all points repulse each other? How to optimize?\n",
    "            $$\\min \\mathcal{L}=\\sum_{i, j} p_{i j} \\log \\frac{p_{i j}}{q_{i j}} = \\sum_{i j} p_{i j} \\log p_{i j}-\\sum_{i j} p_{i j} \\log q_{i j}$$ \n",
    "            $$\\min -\\sum_{i, j} p_{i j} \\log w_{i j}+\\log \\sum_{i, j} w_{i j}$$\n",
    "$w_{i j}$=sim($x_i^d, x_j^d$)-> normalized similarity $q_{i , j}$\n",
    "            <!-- ![image.png](attachment:image.png) -->\n",
    "    + Some materials for different tastes: [High-level visualization](https://www.enjoyalgorithms.com/blog/tsne-algorithm-in-ml); [another high-level visualization](https://www.youtube.com/watch?v=NEaUSP4YerM&t=215s); [Concrete explanation and background in math](https://www.youtube.com/watch?v=MnRskV3NY1k&t=1896s)\n",
    "    + Why do we need t-SNE instead of PCA? See [the image](https://www.astroml.org/_images/fig_S_manifold_PCA_1.png) for an intuitive unserstanding.\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance vs Coefficient Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.96674865]\n",
      " [0.96674865 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "f1 = np.array([1,2,3,4,5])\n",
    "f2 = np.array([1,2.5,3,4.9,4.9])\n",
    "x = np.stack((f1, f2),) # (num_feat, num_data)\n",
    "print((np.corrcoef(x, rowvar=True)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.5   2.55 ]\n",
      " [2.55  2.783]]\n",
      "[2.5   2.783]\n",
      "[1.58113883 1.6682326 ]\n",
      "[[1.         0.96674865]\n",
      " [0.96674865 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "c = np.cov(x, rowvar=True, dtype=None)\n",
    "print(c)\n",
    "d = np.diag(c)\n",
    "print(d)\n",
    "stddev = np.sqrt(d.real)\n",
    "print(stddev)\n",
    "corr = c/stddev[:, None]\n",
    "corr = corr/stddev[None, :]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.40242142 0.14923182 0.12059623]\n",
      "0.6722494722420844\n"
     ]
    }
   ],
   "source": [
    "# Measure the reconstruction error\n",
    "# n_comp = range(1, 10)\n",
    "# rec_error = np.zeros(len(n_comp))\n",
    "# for k in n_comp:\n",
    "#     pca = PCA(n_components=k)\n",
    "#     Zred = pca.fit_transform(Xnorm)\n",
    "#     Xrec = pca.inverse_transform(Zred)\n",
    "#     rec_error[k-1] = np.linalg.norm(Xnorm-Xrec, 'fro')/np.linalg.norm(Xnorm, 'fro')\n",
    "#     # print(\"k={}, rec_error={}\".format(k, rec_error[k]))\n",
    "# #Visualize the change in error\n",
    "# plt.plot(n_comp,rec_error)\n",
    "# plt.xlabel('No of principal components (k)')\n",
    "# plt.ylabel('Reconstruction Error')\n",
    "# np.linalg.norm(Xnorm-Xrec, 'fro')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp260",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a76e165ef781bae0213771e3c52fcb1dfc11aa754df214bdcb59aab9db239e7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
